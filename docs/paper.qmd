---
editor: source
title: |
  | How Does the Accounting Research Community
  | Think About Open Science?^[Joachim Gassen is based at Humboldt-Universität zu Berlin, while Jacqueline Klug and Victor van Pelt are affiliated with WHU - Otto Beisheim School of Management. We thank Kris Hardies, Stephan Hollander, and Maximilian Müller, as well as audiences at University of Antwerp, BI Norwegian Business School, Humboldt University Berlin, Stanford University, the TRR 266 Annual Conference, and WHU for providing helpful feedback. Scott Summers and David Wood thankfully provided data from the BYU Accounting Research Rankings. Excellent research assistance by Benedikt Hahn, Fabian Kalife, and Timnah Weckner is gratefully acknowledged. This study was approved by the WHU Ethical Review Board and supported by Deutsche Forschungsgemeinschaft (Project-ID 403041268, TRR 266 Accounting for Transparency).]
author: 
  - name: Joachim Gassen
    email: gassen@wiwi.hu-berlin.de
    affiliation: Humboldt-Universität zu Berlin
  - name: Jacqueline Klug
    email: Jacqueline.Klug@whu.edu
    affiliation: WHU Otto Beisheim School of Management
  - name: Victor van Pelt
    email: Victor.vanPelt@whu.edu 
    affiliation: WHU Otto Beisheim School of Management
date: last-modified
date-format: long
execute:
  echo: false
  message: false
  warning: false
format:
  pdf:
    documentclass: article
    template-partials:
      - materials/title.tex
    number-sections: true
    colorlinks: true
    papersize: letter
    fig-pos: H
    fig_caption: yes
    fig-cap-location: top
    geometry: margin=1in
    fontsize: 11pt
    ident: yes
ourabstract: We explore the attitudes of the accounting research community towards open science practices by surveying a sample of scholars who have published in leading accounting journals. Building on a discussion of the costs and benefits of open accounting research, we find that accounting researchers, compared to peers from other social science disciplines, are more skeptical about the reproducibility of their influential findings. Additionally, they state to be less familiar with publicly sharing research materials and to pre-register their study designs less frequently. Despite believing that both the accounting research community and journal editors support the sharing of research materials, there is skepticism regarding its actual prevalence. To supplement these survey insights, we analyze observational data, revealing that research material sharing is relatively uncommon and predominantly associated with journals enforcing data and code sharing policies. Finally, our data suggests that accounting researchers view research material sharing as yielding low private benefits but high private costs. We interpret our evidence as indicative of an "accounting open science dilemma" and propose strategies for effectively addressing this issue.

bibliography: references.bib
biblio-style: apsr

always_allow_html: yes

header-includes:
  - \usepackage{setspace}\doublespacing
  - \setlength{\parindent}{4em}
  - \setlength{\parskip}{0em}
  - \setlength{\skip\footins}{2em}
  - \usepackage[hang]{footmisc}
  - \setlength{\footnotemargin}{1em}
  - \usepackage{pdfpages}
  - \usepackage{pdflscape}
  - \usepackage{csquotes}
  - \usepackage{lscape}
  - \usepackage{graphicx}
  - \usepackage{pdflscape}
  - \renewcommand{\mkbegdispquote}[2]{\itshape}
  - \usepackage{longtable}
  - \usepackage{geometry}
  - \geometry{a4paper, margin=1in}
  - \usepackage{setspace}
---

```{r setup, include=FALSE}
library(knitr)
library(rmarkdown)
library(gt)
library(modelsummary)
# opts_chunk$set(fig.pos = 'p') # Places figures on their own pages
opts_chunk$set(out.width = '80%', dpi=600)
opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, cache = FALSE)

source("../code/table_functions.R")
source("../code/figure_functions.R")

sr <- readRDS("../data/generated/survey_response.rds")
mr <- readRDS("../data/generated/merged_response.rds")
vr <- readRDS("../data/generated/verbal_response_coded.rds")

vars <- read_csv("../data/external/variables.csv", show_col_types = FALSE)
pop <- read_csv("../data/external/invited_authors_anon.csv", show_col_types = FALSE)
byu <- read_csv("../data/external/byu_author_data_anon.csv", show_col_types = FALSE)

authors <- read_csv("../data/external/obs_data_authors_anon.csv", show_col_types = FALSE)
papers <- read_csv("../data/external/obs_data_papers_anon.csv", show_col_types = FALSE) 

sr$exp_tment_collab <- 1 - sr$exp_tment_control
svr <-sr %>% left_join(vr %>% select(-exp_reflect), by = "id") 

modh1 <- lm(exp_pct_share ~ exp_tment_collab, data = sr)
modh2 <- lm(exp_pct_share ~ exp_tment_private, data = sr)
modh3 <- lm(exp_pct_share ~ exp_tment_collab*exp_tment_private, data = sr)

source("../code/obs_data_analyses.R")

gt_add_header_mpage_table <- function(x) {
  x <- sub("\\\\midrule", "\\\\midrule \\\\endhead", x)
  x
}
```

::: {.content-visible when-meta="paper-titlepage"}

```{=tex}
\thispagestyle{empty}
\begin{abstract}
\singlespace
```

{{< meta ourabstract >}}

```{=tex}
\end{abstract}
\vspace{0.5cm}
\begin{flushleft}
\textbf{Keywords}: Accounting Research, Open Science, Reproducibility, Research Material Sharing, Pre-Registration, Survey \newline
\textbf{Data Availability}: Data available in code repository \newline
\textbf{Code Repository}: \href{https://github.com/joachim-gassen/osacc}{https://github.com/joachim-gassen/osacc}  \newline
\textbf{Survey Dashboard}: \href{https://jgassen.shinyapps.io/osacc/}{https://jgassen.shinyapps.io/osacc} \newline
\textbf{Declaration of Interest}: The author(s) declare(s) that they have no conflict of interest \newline
\end{flushleft}
```

:::

::: {.content-visible when-meta="paper-body"}

```{=tex}
\pagebreak
\setcounter{page}{1}
```

::: {.content-visible when-meta="anonymize"}

```{=tex}
\begin{center}
\Large
\textbf{How Does the Accounting Research Community\\Think About Open Science?}
\normalsize
\end{center}
\begin{abstract}
\singlespace
```

{{< meta ourabstract >}}

```{=tex}
\end{abstract}
\vspace{0.5cm}
```

:::

## Introduction


Cumulative research relies on reproducibility, which refers to the ability to recreate the results of previous studies using the same materials as the original researchers [@NSF_2015].[^1] Across all fields of science [@B_2016; @CDF_2016; @M_2017] and within accounting [@HLL_2020; @G_2023; @S_2023; @CGL_2024], scholars have diagnosed shortcomings in reproducibility and advocated the use of various open science methods. Open science implies conducting research in ways that allow others to collaborate and contribute. Two key open science methods proposed to enhance the reproducibility of research include the public sharing of research materials and the pre-registration of study plans. In recent years, there has been a noticeable increase in the adoption of these open science methods in social sciences [@FLCPSWMBP_2023].

[^1]: [We received institutional approval for this study.]{.content-visible when-meta="anonymize"} The terms "reproduction" and "replication" are at times used as synonyms but have different meanings. Reproduction refers to achieving the same results of a prior study using the same materials (code, procedures, and data), while replication refers to duplicating the results of a of a prior study using the same code and procedures but new data [@NSF_2015; @BCKU_2024]. For a more in-depth discussion of the differences and related terminological debate, see @DJ_2024.

The sharing of research materials and the use of pre-registrations can be understood as disclosure mechanisms that enable researchers to contribute public goods to the academic community while also signaling the quality of their work. Despite their potential benefits, researchers may rationally under-engage in these activities due to limited private benefits and substantial direct and indirect costs. This could lead to a socially sub-optimal provision of public research materials causing a general lack of research transparency. Starting from this conceptual framework, we conduct a survey among accounting researchers to explore their perceptions and attitudes towards open science.

Based on the responses of `r nrow(sr)` published accounting researchers, our findings reveal that the accounting community's attitudes towards open science clearly differ from the views of other social scientists. Notably, accounting researchers exhibit a generally more pessimistic perspective on the reproducibility of their domain's influential work. In contrast to economists and sociologists, for instance, accounting researchers are approximately half a point more skeptical on a 5-point Likert scale. Both analytical and qualitative researchers demonstrate heightened levels of skepticism. Analytical researchers are particularly critical of research employing non-analytical methods. This indicates that methodological differences might influence perceptions of research reproducibility within the field of accounting.

Given these findings, it could be expected that accounting researchers would adopt a proactive stance and embrace open science practices to enhance research reproducibility. However, our data reveal that, compared to their counterparts in other social science disciplines, accounting researchers state to be less familiar with the practice of publicly sharing research materials and assign it less importance. Respondents using archival, field, and experimental methods demonstrate the highest familiarity with this practice. Conversely, those employing qualitative methods report both the least familiarity and a diminished perception of its significance. This suggests that methodological orientation may influence attitudes toward the adoption of open science practices within the field of accounting.

Similar to, albeit even more pronounced than, other social science researchers, accounting researchers exhibit a disparity between their perceived importance of sharing research materials (prescriptive), their willingness to do so (stated), and their perception of actual sharing practices within their community (descriptive). While 57% of respondents report that they at least sometimes share their research materials publicly, only about 44% believe that this practice is favored among their peers. Interestingly, they estimate that 61% of top-six accounting journal editors support material sharing. This might suggest a generally supportive stance within the community. However, the view of the reality is more sobering: when asked about actual sharing behavior, our respondents estimate on average that only 27% of accounting researchers actually publicly share research materials. These perceptions vary significantly across methodological areas too. Analytical researchers demonstrate the narrowest gap, expecting 43% of accounting researchers to be in favor of sharing, while estimating that 38% actually do so. In contrast, archival researchers report the widest gap, with 45% expected to support sharing but only 24% expected to engage in it. Qualitative researchers report the lowest figures overall, with just 36% perceiving support for sharing and estimating that only 18% engage in the practice. This variability underscores a significant disparity between the perceived and actual sharing of research materials in accounting academia, particularly across different methodological approaches.

In terms of pre-registration, accounting researchers exhibit a familiarity with the concept similar to that of their peers in other social sciences. Despite this, they report adopting pre-registration practices approximately two years later than their counterparts and using it less often, with only 17% practicing pre-registration compared to 27% in other fields. While their trust in pre-registered work aligns with that of other social science researchers, it notably falls short of the high trust levels seen in Psychology, where pre-registration is most prevalent.


To contrast these perceptions with actual behavior, we collected additional observational data from a country-stratified random sample of `r nrow(authors)` authors within our target population. Our findings indicate that 39% of these authors engage in some form of research material sharing, significantly lower than the 57% of our respondents claiming that they at least sometimes share research data but higher than the skeptical mean of 27% that they estimate for the actual sharing rate. A deeper analysis of the observational data reveals that in accounting, material sharing often appears as an occasional compliance measure. Specifically, only 7.5% of the papers authored by individuals in this dataset are published with accessible research materials. Also, when we exclude Journal of Accounting Research and Management Science, two journals with data and code sharing policies, the share of authors for which we document at least some form of research material sharing drops from 39% to 23%. Furthermore, we document relatively few pre-registrations in our observational data, and all are linked to the Registered Report Initiative of the Journal of Accounting Research.

In light of these findings, a logical next step is to investigate the primary rationales for and against sharing research materials. We approach this through a brief experimental design involving a hypothetical scenario.[^2] In the scenario, accounting researchers were asked how likely they would be to share their research materials with a fellow researcher upon request. They reported an average likelihood of 72%. The probability of sharing was unaffected by the randomized rationale provided (i.e., whether the fellow researcher expressed an intent to collaborate or control) or the decision's scope (i.e., whether the fellow researcher requested to share the research materials publicly versus privately). Their reflections reveal that research material sharing in the hypothetical scenario is driven by a belief that it enhances both the quality of respondents' research and of research in general. Additionally, there is a desire to support fellow researchers. The most frequently cited reasons for not sharing include the proprietary nature of the data (noted by 56% of respondents), the time required to make materials available (30% of respondents), and the fear of being scooped once materials are public (23% of the respondents). The reasons given for withholding research materials in the hypothetical scenario vary across research methodologies. For qualitative researchers, issues with deidentification are notably prominent (46%). Meanwhile, analytical researchers view the time investment needed to prepare materials as less of a deterrent compared to other researchers (only 9.5% versus 30% overall).

[^2]: We pre-registered this experimental part of our study at the [Open Science Foundation (link omitted for review)]{.content-visible when-meta="anonymize"}[[Open Science Foundation](https://osf.io/frmzt/)]{.content-visible unless-meta="anonymize"}.

Collectively, our findings indicate an "accounting open science dilemma": while accounting researchers express skepticism about the reproducibility of their research, they also question the efficacy of open science methods, such as research material sharing and pre-registration, in improving it. This dual skepticism may explain why these open science methods are currently less established in the accounting domain than in other fields. The assessments vary across methodological paradigms. Qualitative research faces challenges different from archival work, and they both differ again from experimental studies. In the concluding section of our paper, we discuss these challenges and offer recommendations to how the diagnosed dilemma might be overcome.

Our study contributes to the ongoing debate on reproducibility and transparency of accounting research. By building on the survey evidence presented by @HLL_2020, we provide more comprehensive and timely insights into accounting researchers' views regarding the reproducibility of their influential work, as well as the perceived advantages and disadvantages of open science practices. Furthermore, our study benchmarks these findings against those from related disciplines within the social sciences, facilitating a rough comparative analysis of how accounting researchers' perceptions align or differ from their peers in economics, psychology, sociology, and political science. Additionally, we corroborate recent evidence concerning the robustness of empirical work in accounting [@G_2023; @S_2023; @LSA_2024; @CGL_2024] and management science [@FGHKO_2023] by delving deeper into potential reasons why the discipline of accounting may, or may not, face issues with reproducibility.

## Expectations and Study Design

### Costs and Benefits of Open Accounting Research

Many of the arguments that have been crafted around the costs and benefits of open science in general [@MS_2019; @AM_2019; @ABC_2021; @G_2022; @FLCPSWMBP_2023; @BCKU_2024] also apply to the field of accounting. Below, we present and extend some of these thoughts in a voluntary disclosure-inspired framework [@LT_2005; @MS_2019; @BCLW_2010]. For this purpose, our discussion will be primarily anchored on research material sharing (the main focus of our study) but should largely extend to pre-registration as well. Understood as a disclosure activity related to underlying research, publicly available research materials constitute a public good. To the extent that they are reusable, they accelerate scientific discoveries, improving the quantity and, most likely, also the quality of research. Even if they are not reusable, they still reduce the costs of reproducing the original work, thereby reducing assurance costs. In analogy to classic auditing research [e.g., @S_1980]: reduced assurance costs should lead to an overall increase in assurance activities, ultimately resulting in better research quality.

This means that, similar to other disclosure activities, sharing research materials does not necessarily benefit the producer but the research community at large. While intrinsic motivation for good research can be expected to be a strong determinant of individuals self-selecting into the academic accounting profession, one can still expect accounting researchers to under-engage in research material sharing because the production costs are mostly private while the benefits are mostly public. So, are there any private benefits to sharing research materials? Besides the warm glow of helping others, research material sharing likely improves the quality of the sharing individual's research output. First, it increases the likelihood that mistakes, if present, are being detected. As such incidents likely impose a cost on sharing researchers, this should increase research care prior to sharing. Second and related, if research materials are shared relatively early, the sharing researcher can directly benefit from community feedback prior to the ultimate publication of the study. Uncovered mistakes can also be an important learning opportunity, improving research quality in the long run.

Based on these arguments, there are reasons to believe that voluntary research material sharing can act as a signal for research quality, creating an incentive for researchers to use it to signal the quality of their work in the publishing and career process. In ideal circumstances, we should obtain the well-known unraveling result where researchers even share and improve low-quality research because the research community would otherwise assume the worst [@M_1981]. One critical obstacle to the unraveling result are disclosure costs [@V_1983], also known as proprietary costs in the disclosure literature. For research materials sharing to unravel, it is important that the costs of sharing are sufficiently low, especially for researchers that produce high-quality work.

The direct costs of research material sharing can be sizable as preparing data, code and research procedures for dissemination requires care, and care requires time. While care will also improve quality (see above), at least some of the documentation and code refactoring work to increase accessibility and reusability will not directly pay off on the quality of the underlying work. Another important factor to consider is the cost of maintaining publicly available materials. While some replication packages or similar materials are static in nature and require only limited maintenance, most data and code packages require ongoing updating and support activities. One important aspect of maintenance costs is that they increase significantly with public demand. As demand is directly linked to the scientific impact of the underlying work, high maintenance cost sharing activities are likely to also be high parivate and public benefit activities.

The indirect costs of research material sharing are, first and foremost, linked to the quality assurance process outlined above. Flaws in publicly shared research materials can trigger behavioral (shame and regret) and reputation costs. Specifically, accounting researchers may implicitly assume that reproduction failures could lead others to suspect questionable research practices. However, this suspicion might be unwarranted, as failed reproductions can result from honest mistakes. Another important aspect are proprietary costs from scientific competition. While likely to be positive from the societal perspective, the indirect individual costs of being "scooped" for an interesting follow-up project can be sizable. Finally, there is also the possibility of unethical misuse of shared research materials. Malevolent researchers might use shared materials without proper attribution or use it with a strategy of "reverse p-hacking", meaning creating reproductions or replications that communicate a biased picture of the overall evidence to increase their chances for publication.

In this simplified disclosure framework, rational researchers can be expected to voluntarily share research materials when their private benefits outweigh their private costs. Whether this happens will critically depend on whether the signaling mechanism outlined above unfolds. In a credence good market like research, signaling is a key mechanism to avoid market failure. Based on the discussion above, a critical aspect could be the proprietary cost of competition, as one could argue that this cost is higher for high quality researchers with innovative ideas than for low-quality researchers who will likely face limited demand for their materials. Also, the risks of reputation costs or misuse of materials could be higher for high-quality researchers. On the other hand, it can be argued that direct costs should be lower for high-quality researchers because of superior skills. Finally, outside options should be generally higher for high-quality researchers, meaning that they face higher opportunity costs of their time but also less competitive pressure to keep their research materials private.

The discussion should sound familiar to those who study voluntary disclosures by firms. What is different from our traditional firm-setting is that most researchers are not acting as entrepreneurs. Instead, they are employed by academic institutions or act as civil servants. This can, to some extent, mute the relevance of the benefits and the costs outlined above, at least for researchers with tenured positions. In addition, the employing institutions might also interact with the decision of whether or not to make research materials available. They are designing performance and tenure evaluation criteria that incentivize researchers. Also, they compensate researchers for their time and they provide them with infrastructure that influences and, at times, drastically reduces the direct costs of research material sharing. They might even hold property rights on the intellectual capital that their employees generate. Based on the public announcements of many leading academic institutions, it seems as if these are generally advocating the positive externalities of research material sharing. However, the incentive systems designed by these institutions appear at least in parts inconsistent with these announcements, as they value individual research achievements in form of publications and only rarely publicly available research materials.

Finally, publishers and journals are influential in research material sharing decisions as well. Leading journals have established rigorous data and code sharing guidelines, essentially transforming the voluntary disclosure decision into a mandatory disclosure requirement. While it is interesting to question how journal competition affects the dissemination of data and code sharing guidelines, it seems as if imposing mandatory disclosure rules is a classic response to overcome the diagnosed public good problem.

### Research Questions and Survey Design

Following the discussion above, we designed our survey to address three overlapping research questions.

1.  How does the accounting research community assess the reproducibility of their influential work?

2.  What are the views of the accounting research community about research material sharing and pre-registration, two prominent open science methods to increase the reproducibility of research?

3.  What are the (perceived) costs and benefits of research material sharing?

The first two questions are descriptive in nature. We compare the response of our sample with responses by researchers from other social science fields. While the first question sets the stage by assessing whether our participants sense accounting research to face a reproducibility crisis, the second question explores whether accounting researchers believe that research material sharing and pre-registration are viable strategies and sufficiently common to increase the reproducibility of research. Here too, comparing the views of accounting academics with the views of neighboring fields is essential for a comprehensive understanding of these practices within the wider academic community.

The third question strives to understand the potential causes behind these assessments. Using a small experimental component within our survey, we ask respondents how likely they would be to share their research materials with a fellow researcher. We examine whether respondents' sharing likelihood depends on two aspects of a hypothetical sharing request by a fellow researcher, namely, its rationale (i.e., whether the fellow researcher expressed an intent to collaborate or control) and its scope (i.e., whether the fellow researcher requests to share the research materials publicly versus privately). Our motivation for the treatments is that the rationale should shift the focus from benefits (collaboration) to costs (control) while the scope treatment should manipulate the salience of indirect costs. Besides studying the effect of these two experimental treatments, we also use structured and unstructured survey response data to uncover perceived benefits and costs from research material sharing.

To verify whether the assessments provided by our survey participants are consistent with their behavior in the field, we also conduct an observational study that uses data from 100 authors of our target population to contrast the survey response with the actual research material sharing and pre-registration behavior in the field.

To strengthen the descriptive appeal of our findings by ensuring their comparability with those from other social science disciplines, we modified key questions from the "Survey of Open Science Practices and Attitudes in the Social Sciences" [@FLCPSWMBP_2023], published in Nature Communications.[^3] Like our survey, this study collects insights from published researchers and PhD students on topics such as reproducibility, sharing research materials, and pre-registration. However, due to differences in the scope and target audience, some adjustments were necessary for our study. While the referenced study [@FLCPSWMBP_2023] involved a 45-minute session with participant compensation, our survey is anonymous, designed for completion in under 10 minutes, and offers mild incentivization through donations to UNICEF. Our changes allow us to significantly condense the survey while ensuring relevance to accounting researchers and their established research practices. We pre-tested our adapted questions among a group of accounting doctoral students, and no issues were reported. [Online-]{.content-visible when-meta="online-appendix"}Appendix D provides a comprehensive matching table that contrasts each adapted question and its corresponding scale with the original questions used by @FLCPSWMBP_2023.


[^3]: The data of @FLCPSWMBP_2023 has been collected in two waves (2018 and 2020) while we collected our data towards the end of 2022. While it is plausible that the passage of time has led to increased awareness and adoption of open science practices among researchers, including those in accounting, the findings of @FLCPSWMBP_2023 document that the increasing time trend in the social sciences has faded out towards the end of their survey period. In addition, an increasing awareness and adoption of open science methods over time would not undermine our main findings that accounting research faces an Open Science dilemma. Specifically, compared to their peers, we find accounting researchers are more skeptical about the reproducibility of their work and lag behind in adopting two key Open Science practices designed to address these concerns even though they are surveyed two years later.


Our survey was developed using oTree [@CSW_2016]. The first stage of the survey includes questions about respondents' views on publicly sharing research materials, which we describe as comprising data, code, and research procedures. Specifically, we ask respondents about their familiarity with the public sharing of research materials and their views of its importance. We also ask them about their perceptions of the prevalence of sharing in the accounting research community, how often they tend to share, and whether they plan to share in the future. For those who indicate less than consistent sharing practices, we further explore the reasons for not sharing. We also ask respondents to estimate the proportion of accounting researchers who support sharing and to gauge what percentage of editors from the top six accounting journals believe that sharing enhances a manuscript's value.

The second stage of the survey features a selection of questions about respondents' views on reproducibility and pre-registration, which we also borrowed and adapted from @FLCPSWMBP_2023. First, we ask respondents about their confidence in the reproducibility of accounting research. We also asked them about their subfield within the domain of accounting (i.e., Auditing, Financial Accounting and Reporting, Managerial Accounting, Taxation, Accounting information systems, and Other) and whether they believe the findings in their field are reproducible. Next, we ask the same questions for their methodological area (i.e., Analytical, Archival/Field, Experimental, Qualitative, Survey, and Other).

After eliciting respondents' perceptions on reproducibility, a set of questions explores their familiarity and experience with the practice of pre-registration. Lastly, we ask respondents about their perceptions of the trustworthiness of pre-registered research compared to non-pre-registered research. The survey ends with a few demographic questions. For a complete overview of the survey questions included in our study, please see [Online-]{.content-visible when-meta="online-appendix"}Appendix A. All answers can be explored using our [[survey dashboard](https://jgassen.shinyapps.io/osacc/)]{.content-visible unless-meta="anonymize"}[survey dashboard (link omitted for review)]{.content-visible when-meta="anonymize"} and the data is available in our public [[GitHub repository](https://github.com/joachim-gassen/osacc/)]{.content-visible unless-meta="anonymize"}[GitHub repository (link omitted for review)]{.content-visible when-meta="anonymize"}.

### Survey Administration and Response Sample

To recruit respondents for our survey, we targeted authors who published in the "Top 6" accounting journals between 2016 and 2021.[^4] These journals include Accounting, Organizations and Society, Contemporary Accounting Research, Journal of Accounting and Economics, Journal of Accounting Research, Review of Accounting Studies, and The Accounting Review.[^5] Following approval from an ethical review board and ensuring the anonymity of participating respondents, we distributed initial survey invitations in November 2022, accompanied by three subsequent reminders. At each point of contact, we provided potential respondents with the option to opt-out from receiving further emails. For details on the invitation emails and all reminders, please refer to [Online-]{.content-visible when-meta="online-appendix"}Appendix A.

[^4]: We started in 2016 as this marks the year when Open Science, and more specifically research reproducibility, became a prominent topic of discussion among academics in the social sciences, including accounting researchers [@OSC_2015; @B_2016; @CDF_2016; @M_2017].

[^5]: We selected these six journals because they are widely regarded as the leading journals within the academic accounting community. While we acknowledge that this is not an exhaustive list and understand there are varying views regarding the ranking of journals in the field, we believe that collectively, these six journals represent a sufficiently broad spectrum of accounting academics for our research purposes.

\centerline{--- Insert Table 1 about here ---}

Table 1 details our sample construction. Out of 2,660 potential respondents identified, 118 were excluded due to operational reasons.[^6] Additionally, 206 respondents' initial emails bounced, indicating unreachable email addresses. Among the remaining 2,336 successfully contacted accounting researchers, 391 initiated the study (15.4% of the total emails sent), 254 partially completed it (10.9% of the total emails sent), and 222 fully completed it (8.7% of the total emails sent). For our analyses, we employ a floating sample, which dynamically changes based on specific criteria, to maximize the use of the available data. For most descriptive analyses, we rely on the number of respondents who completed the entire study (n = 222). However, for certain descriptive analyses, depending on the specific question, we use a broader sample (n = 229) as seven respondents decided to leave out some of the demographic questions. Since our survey experiment component forms the first part of the instrument, we use the sample of respondents who completed the study up to that point (n = 254) for analysis.

[^6]: We removed 41 authors because we found no email address, 41 authors because they appeared in the data twice with two author IDs but were the same person, 28 authors because they were a current member of one of our affiliated universities or academic research centers, 5 authors because they gave us advice for this study, 2 authors because they had passed away, and 1 author due to an incorrect email address.

\centerline{--- Insert Table 2 about here ---}

Table 2 displays response demographics and characteristics based on a sample of up to 229 observations. Panel A reveals that the majority of the respondents are male, 35 years or older (87.8%), specializing in Financial Accounting and Reporting or Managerial Accounting (70.3%), and has Archival or Field Research as their primary research method (55.9%). Panel B of Table 2 contrasts our respondent sample with our sampling population. Given the anonymity of our response and the data that we can collect about our population, we can only directly compare the geographic dispersion of our sample. Panel B reveals that, in particular, researchers from North America (65.2%) and Europe (16.5%) are over-represented, and researchers from Asia are under-represented in the sample. Given that response rate assessment is inherently challenging, we generally avoid speculating about potential reasons for this sampling imbalance. However, it seems likely that the study's all-European author team might have positively influenced the European response. While we control for geographical effects in our response (not-tabulated but available using our [[survey dashboard](https://jgassen.shinyapps.io/osacc/)]{.content-visible unless-meta="anonymize"}[survey dashboard (link omitted for review)]{.content-visible when-meta="anonymize"}) and find only mild variation across regions (researchers from North America tend to state higher sharing activities and preferences), we conclude from this sampling imbalance that our results mostly speak to the attitudes of accounting scholars from North America and Europe and that we can say very little about researchers located in Asia and other geographical regions that happen to have low response rates.

We further evaluate the representativeness of our sample by comparing it to a sample of published authors on the BYU accounting rankings platform [@SW_2023]. Specifically, the BYU accounting rankings rank individual authors based on classifications of peer-reviewed articles in 12 accounting journals since 1990. We contacted the team behind the BYU rankings, and they generously made their data available to us. The data includes `r format(nrow(byu), big.mark = ",")` authors and information on their methodologies as well as subfields. As these data are based on the papers contained in the BYU database as well as by self-classification by authors, one author can be associated with several subfields and methodologies in the BYU data. Therefore, we compare 2,114 classified methodologies in the BYU sample to 229 primary methodologies in our sample. Also, we compare 2,877 classified subfields in the BYU sample to 229 primary subfields in our sample.

\centerline{--- Insert Figures 1 and 2 about here ---}

Figure 1 reveals the frequency distributions of subfields in the BYU and our sample. We can observe some differences between these two distributions. First, our sample has disproportionately more mentions of authors publishing in financial accounting and reporting and managerial accounting than the BYU sample (49% versus 36%). As a result, we see disproportionately fewer authors working in the other subfields in our sample. A chi-square test confirms the frequencies of the two distributions significantly vary (chi-square = 36.415, two-tailed p-value \< 0.001). Figure 2 reveals the frequency distributions of the methodologies in the BYU and our sample. These two frequency distributions are very similar; a little over half of the observations fall into the Archival/Field category for both samples. About one-fifth of the sample falls into the experimental category, and the rest is comprised of analytical and other methodologies. A chi-square test does not reject the null that the two frequency distributions are the same (chi-square = 2.270, two-tailed p-value = 0.518). We conclude from these comparisons that our respondents reflect the methodological composition of accounting research reasonably well.

## Findings

### Reproducibility

The first research question of our study investigates how accounting researchers perceive the reproducibility of influential findings within their field. Table 3 offers a detailed summary of our respondents' answers to the question, "How confident are you that the influential findings in the area of accounting overall are reproducible?" The table also contrasts the answers with those of researchers in other disciplines, as documented by @FLCPSWMBP_2023, based on their question, "How confident are you that the influential research findings in \[discipline\] would replicate?" We deliberately adjusted the wording from "replicate" to "reproduce" to enhance clarity for our audience, recognizing the subtle distinctions between these terms across various research methodologies. The concept of "reproducibility" serves as a central theme in discussions throughout the social sciences, including accounting [@B_2016; @CDF_2016; @M_2017; @HLL_2020]. It signifies the ability to recreate the results of prior studies using the same materials, procedures, and data as employed by the original researchers [@NSF_2015]. In contrast, the term "replication" carries different interpretations across fields. For experimental researchers, it typically refers to conducting a new study with different participants, while for archival researchers, it involves re-analyzing the same or similar data using identical procedures. Notably, the definition provided by @FLCPSWMBP_2023 for replication—"a test of whether new studies on the same topic in a broadly similar sample support the same results"—aligns closely with what we conceive as the common understanding of reproduction in accounting. If anything, our notion of reproduction appears to be less stringent than the replication definition offered by @FLCPSWMBP_2023.

\centerline{--- Insert Table 3 about here ---}

The findings presented in Table 3 indicate that accounting researchers generally harbor a more critical perspective on the reproducibility of their field's influential findings. Specifically, compared to their counterparts in economics and sociology, accounting researchers demonstrate a heightened level of skepticism regarding the reproducibility of their significant work, with an observed difference of approximately half a point on a 5-point Likert scale. This disparity appears less pronounced when accounting researchers are compared to those in political science and psychology. However, using two-sample t-tests for assessing mean differences across fields, we find that Accounting academics are significantly more skeptical compared to those of the other four fields (all two-tailed p-values \<= `r myfmt(max(ct_pvalue(mr, "reprod", "demog_discipline", "Accounting", format = FALSE), 0.001))`). This is consistent with accountants being more concerned about research quality, hinting at the existence of a credence-good problem that open science methods are designed to address.


\centerline{--- Insert Figure 3 about here ---}

Figure 3 graphically illustrates the prevailing skepticism, revealing that 10.5% of accounting researchers express "Not at all confident" attitudes toward the reproducibility of influential findings in their field. This percentage starkly contrasts with the lower levels of skepticism observed among economists (0.7%), political scientists (2.1%), psychologists (1.1%), and sociologists (1.3%) as reported by @FLCPSWMBP_2023. Additionally, the figure indicates that accounting has the lowest proportion of non-skeptics—those moderately or extremely confident in the reproducibility of key findings—with only 39.3% falling into this category. In comparison, 61.1% of economists, 45.8% of political scientists, 41.3% of psychologists, and 61.7% of sociologists exhibit greater confidence.

To gain a more nuanced understanding, we examine the assessments of reproducibility across various methodologies employed within the field of accounting. Accounting is characterized not only by its diversity in orientation and subject matter but also by the range of methodologies adopted by its researchers. We categorize these methodologies into five distinct groups: Analytical, Archival/Field, Experimental, Qualitative, and Survey. We asked respondents to identify which of these categories best described their primary methodological approach, allowing for clear categorization in our dataset. Table 4 presents their evaluations regarding the reproducibility of influential findings, organized into three panels. Panel A offers insights into reproducibility assessments within accounting as a whole, Panel B delves into evaluations specific to each respondent's main methodological area, and Panel C highlights the differences in assessments between accounting at large and their primary methodological focus.


\centerline{--- Insert Table 4 about here ---}

The findings presented in Table 4 Panel A highlight that both analytical and qualitative researchers exhibit a more skeptical outlook regarding the reproducibility of influential findings in the field of accounting compared to their counterparts employing different methodologies. Notably, analytical researchers demonstrate significantly lower confidence in the reproducibility of such findings in general when contrasted with those utilizing Archival/Field (`r ct_pvalue(sr, "reprod_acc", "demog_method", "Analytical")["Archival/Field"]`) and Experimental methodologies (`r ct_pvalue(sr, "reprod_acc", "demog_method", "Analytical")["Experimental"]`). Conversely, when assessing their own methodological domains in Table 4 Panel B, analytical researchers show the highest levels of confidence, with their average confidence being statistically greater than that reported by peers following the other four methodologies (all two-tailed p-values \<= `r myfmt(max(ct_pvalue(sr, "reprod_method", "demog_method", "Analytical", format = FALSE), 0.001))`). This indicates a notable discrepancy between their perception of the field at large and that of their specific research approaches. Additionally, qualitative researchers report significantly lower average confidence than those engaged in Analytical (`r ct_pvalue(sr, "reprod_method", "demog_method", "Qualitative")["Analytical"]`), Archival/Field (`r ct_pvalue(sr, "reprod_method", "demog_method", "Qualitative")["Archival/Field"]`), and Experimental Research (`r ct_pvalue(sr, "reprod_method", "demog_method", "Qualitative")["Experimental"]`). However, their confidence levels are similar to those in the Survey category.

Panel C introduces the concept of the "assessment gap," measures the difference in assessments of the reproducibility of influential findings in respondents' own methodological area versus accounting as a whole. A larger assessment gap signifies greater confidence in the reproducibility of influential findings within one's own methodology relative to the broader field of accounting. The results displayed in Panel C reveal that analytical researchers exhibit the most substantial assessment gap, indicating a pronounced skepticism regarding the overall reproducibility of the field compared to their specialized methodological practices. Moreover, the average assessment gap for the Analytical category is significantly different from the averages reported across the other four categories (all two-tailed p-values \<= `r sr$reprod_method_delta <- as.numeric(sr$reprod_method) - as.numeric(sr$reprod_acc);  myfmt(max(ct_pvalue(sr, "reprod_method_delta", "demog_method", "Analytical", format = FALSE), 0.001))`). In contrast, no significant differences are observed among the other methodological categories, suggesting a more uniform view across different research approaches within accounting.

### Sharing Research Materials - Familiarity and Importance

Given the observed skepticism and as discussed in Section 2, one might anticipate that accounting researchers would adopt a proactive stance on open science methodologies. In line with the proverb by Supreme Court Justice Louis Brandeis, "sunlight is the best disinfectant," it seems reasonable to expect accountants to advocate for increased transparency in research practices. To evaluate this notion, the survey poses two questions regarding the practice of sharing research materials, which it defines as the practice of sharing data, code, and research procedures. These questions are closely adapted from the survey conducted by @FLCPSWMBP_2023, allowing for a comparative analysis of perspectives across fields.

\centerline{--- Insert Table 5 about here ---}

The first question evaluates respondents' familiarity with the practice of sharing research materials. Table 5, Panel A reveals that accounting researchers exhibit lower familiarity with research materials sharing compared to their peers in other social science disciplines. In general, academics within the social sciences report a reasonable degree of awareness and familiarity with this practice. However, our findings indicate that accounting researchers are significantly less familiar with it than those in all four other social science fields surveyed (all two-tailed p-values \<= `r myfmt(max(ct_pvalue(mr, "sharing_heard", "demog_discipline", "Accounting", format = FALSE), 0.001))`). The second question assesses respondents' perceived importance of research materials sharing. Table 5, Panel B highlights a similar trend: accounting researchers view this practice as less important compared to researchers in Economics (`r ct_pvalue(mr, "sharing_important", "demog_discipline", "Accounting")['Economics']`), Political Science (`r ct_pvalue(mr, "sharing_important", "demog_discipline", "Accounting")['Political Science']`), and Psychology (`r ct_pvalue(mr, "sharing_important", "demog_discipline", "Accounting")['Psychology']`) while viewing it as more important compared to researchers in Sociology (`r ct_pvalue(mr, "sharing_important", "demog_discipline", "Accounting")['Sociology']`).


\centerline{--- Insert Figure 4 about here ---}

Figure 4 provides a graphical representation of these findings, depicting frequencies along horizontal axes and aligning them to a neutral midpoint. In our study, only 39.6% of accounting researchers consider research material sharing "very important." This is notably lower than the percentages observed in other fields: Economics at 51.7%, Political Science at 48.1%, and Psychology at 43.8%. Sociologists report an even lower percentage of strong advocates, at 29.4%, compared to accounting researchers. These variations indicate differing attitudes towards open science across academic disciplines.


\centerline{--- Insert Table 6 about here ---}

We delve deeper into attitudes concerning the familiarity with, and perceived importance of, sharing research materials across different methodologies in accounting. Our analysis again distinguishes between five methodological domains: Analytical, Archival/Field, Experimental, Qualitative, and Survey. Panel A of Table 6 reveals that respondents who primarily use Archival and Field methods exhibit greater familiarity with research material sharing compared to those using Qualitative (`r ct_pvalue(sr, "sharing_familiar", "demog_method", "Archival/Field")['Qualitative']`), Analytical (`r ct_pvalue(sr, "sharing_familiar", "demog_method", "Archival/Field")['Analytical']`), and Survey methods (`r ct_pvalue(sr, "sharing_familiar", "demog_method", "Archival/Field")['Survey']`). Additionally, it is evident that individuals employing Qualitative methods are significantly less familiar with the practice than their counterparts in the other four methodological categories (all two-tailed p-values \<= `r myfmt(max(ct_pvalue(sr, "sharing_familiar", "demog_method", "Qualitative", format = FALSE), 0.001))`). In contrast, Panel B indicates that there are few notable differences in the perceived importance of research material sharing across the various methodologies. The only significant distinction is observed among respondents in the Archival/Field category, who view research material sharing as considerably more important than those using Qualitative (`r ct_pvalue(sr, "sharing_important", "demog_method", "Archival/Field")['Qualitative']`) and Survey (`r ct_pvalue(sr, "sharing_important", "demog_method", "Archival/Field")['Survey']`) methods. Overall, these findings suggest that accounting researchers may not strongly endorse the signaling value of research material sharing. This could be attributed to their relative unfamiliarity with the practice or the perception that its benefits are relatively low while costs appear high.

### Sharing Research Materials - Prescriptive versus Descriptive Views

Next, our survey focuses on the perceived prescriptive and descriptive importance of research material sharing. Table 7 Panel A reports data on the frequency of sharing, revealing that 57% of the respondents state that they at least sometimes publicly share their research materials. We also find that stated sharing tendencies are significantly lower for respondents in the Qualitative (all two-tailed p-values \<= `r myfmt(max(ct_pvalue(sr, "sharing_use", "demog_method", "Qualitative", format = FALSE)[c("Analytical", "Archival/Field", "Experimental")], 0.001))`) and Survey (all two-tailed p-values \<= `r myfmt(max(ct_pvalue(sr, "sharing_use", "demog_method", "Survey", format = FALSE)[c("Analytical", "Archival/Field", "Experimental")], 0.001))`) categories compared to each of the three other methodological areas, i.e., the respondents in the Analytical, Archival, and Experimental categories.


\centerline{--- Insert Table 7 about here ---}

Table 7 Panel B reports answers to three questions measuring respondents' perceptions of how important others consider research material sharing to be. Namely, these three questions ask (1) what percentage of accounting researchers in general favor research material sharing, (2) what percentage of editors of the top six accounting journals believe it increases the value of a manuscript, and (3) what percentage of accounting researchers publicly share research materials. The data in Panel B reveal that respondents believe about 44% of accounting researchers favor publicly sharing research materials, they believe 61% of the top-six accounting editors are in favor of publicly sharing research materials, and they believe 27% of accounting researchers actually share research materials.


\centerline{--- Insert Figure 5 about here ---}

To visualize the differences between these three questions, Figure 5 displays scatter box plots, one for each question and separately for each methodology. First, examining the three plots for the sample as a whole, a clear visual pattern emerges that the assessments of actual sharing behavior among accounting researchers lag behind assessments of whether accounting researchers favor sharing, which, in turn, lags behind assessments of whether editors value sharing. This pattern is starker for respondents in the Archival/Field category, compared to respondents using analytical (`r sr$sharing_perc_divide <- sr$sharing_perc_editor_prefer - sr$sharing_perc_other_share; ct_pvalue(sr, "sharing_perc_divide", "demog_method", "Archival/Field")['Analytical']`) and experimental methods (`r ct_pvalue(sr, "sharing_perc_divide", "demog_method", "Archival/Field")['Experimental']`). This finding highlights a contrast between the perceived prescriptive importance of research material sharing and the perceived (descriptive) actual sharing behavior. In other words: We believe that we should share and claim we do, yet we do not believe that others do.

### Pre-registration

Our survey has so far revealed that accounting researchers who participated in our survey are more skeptical about reproducibility than other social science areas. We also find that they are less familiar with research materials sharing as an open science practice that helps address reproducibility and that they find that practice less important. Besides research material sharing, our survey also covers a second Open Science practice: Pre-registration.[^7]

[^7]: While parts of the economic [@O_2015; @BCHH_2024] and accounting literature [@CGL_2024] differentiate between pre-registrations (relatively short and standardized descriptions of study design) and pre-analysis plans (more detailed and elaborate plans outlining the technical details of a study, potentially also including analysis code, mock-up tables and figures), we use the term pre-registration as a broader notion, in line with @FLCPSWMBP_2023.


\centerline{--- Insert Table 8 about here ---}

Table 8 summarizes three questions about pre-registration. Panel A considers respondents' familiarity with the practice itself. We asked respondents: "How familiar are you with the practice of pre-registering hypotheses or analyses in advance of a study?"[^8] The results in Panel A suggest that accounting researchers are equally familiar with the practice as researchers in Political Science and Psychology. They seem to be more familiar than researchers in Economics (`r ct_pvalue(mr, "prereg_heard", "demog_discipline", "Accounting")['Economics']`) and Sociology (`r ct_pvalue(mr, "prereg_heard", "demog_discipline", "Accounting")['Sociology']`). While this finding might appear surprising at first glance, one has to factor in that the practice of pre-registration is gaining traction over time, and our survey was fielded two years later than the second wave of our benchmark survey.

[^8]: The question used in the survey of @FLCPSWMBP_2023 is "Have you ever heard of the practice of pre-registering hypotheses or analyses in advance of a study?". To ease comparison of the respective findings, we recode our response data to imply 'No' for participants that answer 'Not at all familiar'.

\centerline{--- Insert Figure 6 about here ---}

Figure 6 reports data on the takeup of pre-registration over time. It is based on the question: "Approximately when was the first time you pre-registered hypotheses or analyses in advance of a study?" The findings show a clear pattern: Accounting researchers started pre-registering their studies significantly later than researchers in other social science fields (all two-tailed p-values \<= `r myfmt(max(ct_pvalue(mr, "prereg_use_first_time", "demog_discipline", "Accounting", format = FALSE), 0.001))`). They started around 2018, compared to 2015-2016 for the other fields.

Panel B of Table 8 presents data on answers to the question: "How trustworthy do you find the work of scholars who tend to pre-register hypotheses or analyses as compared to the work of those who don't?" Results indicate no apparent differences in response in accounting compared to economics (`r ct_pvalue(mr, "prereg_opinion", "demog_discipline", "Accounting")['Economics']`) and political science (`r ct_pvalue(mr, "prereg_opinion", "demog_discipline", "Accounting")['Political Science']`). However, the assessment of pre-registration is less positive in accounting compared to psychology (`r ct_pvalue(mr, "prereg_opinion", "demog_discipline", "Accounting")['Psychology']`) but higher than in sociology (`r ct_pvalue(mr, "prereg_opinion", "demog_discipline", "Accounting")['Sociology']`). However, it should be noted that for this question, the comparability across fields is limited as we asked explicitly about trustworthiness, whereas @FLCPSWMBP_2023 referred more generally on a general opinion.[^9]

[^9]: The question used in the survey of @FLCPSWMBP_2023 is "What is your opinion of pre-registering hypotheses or analyses?". This question is quite broad and uses a five-point Likert scale (Not at all in favor, Moderately not in favor, Neither in favor nor against, Moderately in favor, Very much in favor). We chose to use a more specific question asking about whether pre-registration improves trustworthiness while also relying on a five-point Likert scale (Much less trustworthy, Somewhat less trustworthy, Equally trustworthy, Somewhat more trustworthy, Much more trustworthy).


\centerline{--- Insert Table 9 about here ---}

To conclude our descriptive evidence on pre-registration, we examine differences in familiarity, usage, and trustworthiness of pre-registration across the different methodological areas within accounting. Table 9 displays respondents' frequencies using three panels. Panel A shows that qualitative accounting researchers are less familiar with the practice than accounting researchers belonging to the Archival (`r ct_pvalue(sr, "prereg_familiar", "demog_method", "Qualitative")["Archival/Field"]`), Experimental (`r ct_pvalue(sr, "prereg_familiar", "demog_method", "Qualitative")["Experimental"]`), and Survey (`r ct_pvalue(sr, "prereg_familiar", "demog_method", "Qualitative")["Survey"]`) categories. Yet, there is no significant difference between qualitative and analytical accounting researchers (`r ct_pvalue(sr, "prereg_familiar", "demog_method", "Qualitative")["Analytical"]`). Panel B reports on differences in usage, specifically, the question "`r vars$text[100]`". We find that accounting researchers in the experimental field report the highest usage of pre-registration, while none of the qualitative accounting researchers report using it. Lastly, Panel C reports differences in perceived trustworthiness. We find no clear evidence of differences among the different methodologies in whether they trust pre-registered work more than work that has not been pre-registered.

### Usage of Open Science Methods - Observational Data

To contrast these perceptions with reality, we collect observational data on actual research material sharing and pre-registration for a random country-stratified sample of `r nrow(authors)` authors from our target population. To assess whether these authors share research materials, we systematically search their homepages for evidence and also verify whether they have relevant online accounts linked to their names on platforms such as GitHub, Open Science Foundation, Dataverse, or Zenodo. If they do, we collect whether they share research materials over these channels. In addition, we use Scopus to identify all studies that these authors have published since 2016 and verify whether these publications' DOI URLs reference research material sharing.[^10] We also assess various pre-registration platforms to assess whether our authors have used pre-registration. We were only able to identify for `r sum(by_author$has_prereged)` authors clear evidence that they have pre-registered a study. These cases are linked to JAR's registered report initiative. Therefore, this reminder of this section solely focuses on research material sharing.

[^10]: The data collection manual for this process is available in this project's [[GitHub repository](https://github.com/joachim-gassen/osacc/)]{.content-visible unless-meta="anonymize"}[GitHub repository (link omitted for review)]{.content-visible when-meta="anonymize"}.

\centerline{--- Insert Table 10 about here ---}

Table 10 reports our findings based on this sample of `r nrow(authors)` authors and their `r nrow(papers)` papers. Panel A summarizes the data by author. We find that `r sum(by_author$has_made_resmat_available)` % of our authors share research materials. This number and its corresponding binomial confidence interval (30 to 49 %) are lower that the percentage of respondents who stated in the survey that they at least sometimes share (57 %) but higher than the prescriptive sharing frequency that our respondents attributed to other researchers (27 %).

\centerline{--- Insert Figure 7 about here ---}

As becomes apparent from Panel B, our authors only share research materials occasionally. Only 7.5 % of the paper DOI URLs reference research material sharing. As Figure 7 indicates, this share has mildly increased over time, but even in recent years, the paper-level sharing percentage seems relatively modest.

The journal breakdown in Panel B provides clear evidence that the data and code sharing policies established by the Journal of Accounting Research and Management Science drastically influence research material sharing behavior. This also becomes apparent from Panel A. If one omits research material sharing via these journals, the percentage of sharing accounting researchers drops from 39 % to 23 %.[^11]

[^11]: Untabulated analyses based on small frequencies provide suggestive evidence that, overall, the sharing frequencies are also higher for general economics journals, which generally require code and data sharing, than for journals in the areas of accounting and finance.

Overall, our observational data corroborates the survey findings, indicating that research material sharing in accounting occurs infrequently and is primarily driven by journal requirements.

### Rationales Related to Research Material Sharing - Experimental Findings

We now turn to our third and final research question: An attempt to understand how our participants decide on whether to share research materials. As a first step, we use a small experimental design. For this, we create a hypothetical scenario. The scenario outlines that the participating researcher has just conducted a study and published it in an academic journal. Next, participants are informed that a fellow researcher has contacted them for access to their research materials. Next, we provide four pieces of information about the request made by a fellow researcher. In these statements, we manipulate two between-subjects factors: the rationale behind the request for research materials and the scope of sharing. First, we manipulate the rationale used by the fellow researcher requesting access to the researcher's materials. In the *collaboration rationale* treatment, the fellow researcher requests access to researcher materials to understand the work better and build on it. In the *control rationale* treatment, the request aims to verify and potentially correct the research. Second, we vary whether the fellow researcher requests the research materials to be shared publicly (i.e., *public scope*) or privately (i.e., *private scope*). Respondents were randomly assigned one stated rationale (*control* versus *collaboration*) and one sharing scope (*public* versus *private*). Their willingness to comply with the fellow researcher's request was then measured on a scale ranging from 0% (definitely not) to 100% (definitely). The hypothetical scenario was, like the survey, designed using oTree [@CSW_2016]. An overview of the manipulations and the materials can be found in [Online-]{.content-visible when-meta="online-appendix"}Appendix B.

Building on our discussion in section 2, we predict that using a collaborative versus controlling rationale increases accounting researchers' willingness to share their research materials because it communicates a potential benefit rather than cost in terms of reputation (our first hypothesis). We also predict that a private sharing request would be more favorably received due to lower indirect costs triggered by lower reputation risks (our second hypothesis). Finally, we predict that the effect of a collaborative rationale is larger in the case of a private sharing request than a public sharing request (our third hypothesis). The conceptual reasoning behind this interaction effect is that, besides the additive effects of private sharing and a collaboration rationale, the perceived benefit of sharing under the collaboration rationale should be amplified when the scope of sharing is private rather than public. Namely, for a private scope, a collaboration rationale implies more direct benefits of sharing than for a public scope. We pre-registered our predictions and design at the [[Open Science Foundation](https://osf.io/4akyd)]{.content-visible unless-meta="anonymize"}[Open Science Foundation (link omitted for review)]{.content-visible when-meta="anonymize"}.

\centerline{--- Insert Table 11 about here ---}

Table 11 shows the findings of the hypothetical scenario. On average, we find a high willingness to share research materials in the hypothetical scenario and across different variations of rationale and scope (mean = 72%, s.d. = 28%). However, we do not find statistical support for our three predictions. First, Panel B Column 1 reports no significant effect of using a *collaboration rationale* over a *control rationale* (two-tailed p-value `r myfmt(coef(summary(modh1))[2,4], p = T)`). Moreover, Column 2 reveals no support for a main effect of *private scope* on respondents' willingness to share research materials (two-tailed p-value `r myfmt(coef(summary(modh2))[2,4], p = T)`). Lastly, column 3 shows no support for an interaction effect between *collaboration rationale* and *private scope* (two-tailed p-value `r myfmt(coef(summary(modh3))[4,4], p = T)`). In sum, although we find that respondents express a high willingness to share research materials in our hypothetical scenario, we do not find any evidence it is affected by the way the request for sharing is made. At least in our hypothetical scenario, it does not matter whether the request is made to make the materials private (rather than public) and whether the request is made to promote collaboration as a research community or to verify whether the research has been conducted appropriately.[^12]

[^12]: To shed some light on whether our null results could be the outcome of a low powered test caused by our modest response rate, we report a simulation-based post hoc power analysis in [Online-]{.content-visible when-meta="online-appendix"}Appendix C. We find the post hoc minimum detectable effect size of our sample to be around 12 percentage points for our main effects, reflecting that we would have been able to detect a medium-sized effect (Cohen's d \~0.5) for H1 and H2. Thus, we conclude that our results are not driven by low power.

### Rationales Related to Research Material Sharing - Survey Evidence

Given our experimental findings, we rely on two sources of survey data to shed light on our third research question. First, our survey contains a question asking respondents who report that they do not always publicly share research materials to explain which reasons make them withhold their research materials. Respondents are able to select multiple reasons.


\centerline{--- Insert Table 12 about here ---}

Table 12 Panel A displays the frequencies and percentages for each reason featured and provided in our survey. Aside from reporting the total frequencies for the entire sample, it also (again) splits them up into the five methodological fields (i.e., Analytical, Archival/Field, Experimental, Qualitative, and Survey). The most commonly stated reason that respondents gave for not sharing research materials is that the data are proprietary. 56.3% of all respondents give this reason, ranking first for all methodological areas. This is somewhat surprising because not all methodological areas regularly deal with proprietary data. In a shared second place are three more prominent reasons respondents provided, namely, (1) the time that it takes to make materials available (29.9% of all respondents), (2) the fear of being scooped when materials become publicly available (23.2% of all respondents), and (3) no need for sharing as their data are already publicly available (33.9% of all respondents). For these reasons, there are some notable differences across the different methodologies. For qualitative researchers, deidentification problems are a relatively important reason (45.8%). Analytical researchers, in contrast, consider the time that it takes to make materials available relatively less of a reason than researchers using the other methods (i.e., 9.5%).

Besides this multiple-choice question, we also ask our participants to reflect on their answers in our hypothetical sharing scenario. This second data source is interesting (a) because it provides unfiltered insights on the reasons for and against data sharing, and (b) it might be influenced by our experimental manipulations, despite that it did not affect the sharing likelihood in the hypothetical scenario. To gather these additional insights, we tasked a research assistant, who was not informed about the study's experimental design, to code the answers of the participants. The resulting categories, characterizing the answers and their respective frequencies, are presented in Panel B of Table 12. It also splits the frequencies by the different experimental cells of the experiment.

We first focus on the benefits of research material sharing. Panel B reveals that participants share research materials predominantly because they believe sharing enhances research quality. A typical example of such a statement is the following:

\begin{displayquote}
"The purpose of research is to better understand the world, and replication (and scrutiny) of one's research is an essential element of developing a deep understanding of the world."
\end{displayquote}

While many participants refer to research in general when making this point, there are also responses that focus on the respondents' own work. These argue that research material sharing, by uncovering honest mistakes, can improve the quality of the sharing researcher's work. See, for example, the following statement:

\begin{displayquote}
"Because if there is a shadow of a doubt, I pay the consequences. Moreover, I care about the results--so if they can be improved upon, I am all for it!."
\end{displayquote}

Another prominent argument provided by participants is that research material sharing helps other researchers. This argument is raised significantly more often by participants who faced the collaboration treatment, suggesting that the framing of the situation affects decision-making (`r ct_pvalue(svr, "verbal_pos_helps_other_researchers", "exp_tment_collab", 1)`). We interpret this as evidence that our participants see both the public and private benefits of data and code sharing. The following statement illustrates the importance of research material sharing in helping other researchers nicely:

\begin{displayquote}
"I always make supporting information available to other researchers when requested. It may help them learn our process and expand upon our work."
\end{displayquote}

The main rationales against sharing are related to the materials being considered valuable intellectual property that can potentially be reused for other research projects. A typical example of such a statement reflecting the indirect proprietary costs of data sharing is:

\begin{displayquote}
"There might have been substantial workload flown into collecting data that also will benefit follow-up projects. Giving away the data might harm future projects without remuneration for the collection effort."
\end{displayquote}

Another interesting aspect that does not become apparent from the pre-conditioned analysis in Panel A is that a sizable amount of participants indicate that they are concerned about the unethical use of their research materials. These concerns are related to other researchers using their work for "reverse p-hacking," trying to discredit prior findings to establish a contribution. An example of such a statement:

\begin{displayquote}
"It sounds like they are trying to disprove me. They're on a witch hunt. There's a good chance they are doing this to a lot of people and are, themselves, going to suffer from a file drawer problem. If they re-run 100 people's studies and find that 1 doesn't replicate, that's the one they will publish and discredit and shame the researcher, ignoring the fact that it was likely due to chance, and not noting the 99 who got it right."
\end{displayquote}

But even without reverse p-hacking, research material sharing requests can create career concerns. An example:

\begin{displayquote}
"I would want to share my materials but I would be nervous that I did something 'wrong'  and it would make me look bad if my study didn't replicate. I'm a junior faculty and wouldn't want to do anything to jeopardize my reputation or likelihood of getting tenure."
\end{displayquote}

Another aspect, which is also featured prominently in Panel A, is the time investment required to entertain research material sharing and potential follow-up requests. For example:

\begin{displayquote}
"Many academics, including me, are constantly working under high pressure to deliver performance and short of time to fulfill a large range of commitments. It is simply impossible to entertain each and every such request, especially when replying to the first might lead to feeling the obligation to answer more questions from the requester in the future. A requester can be a relatively inexperienced graduate student exploring a doctoral topic, which has implications for how many questions s/he might have in the future. Thus, the propensity to reply to a request often depends on the status of the requester in the field and the anticipated consequences (or the lack of) of entertaining the request (or not)."
\end{displayquote}

The last aspect of this quote also hints at a rationale commonly featured in the "it depends" category: the requesting person's personality and reputation. An example of this is:

\begin{displayquote}
"It depends on whether I know the person and their reputation in the research community. I think my reputation would be tarnished if I didn't provide but I would also be skeptical of providing someone I didn't know my hand-collected data and code that took years of work to produce."
\end{displayquote}

These statements indicate the potential for selective sharing among well-connected researchers, which would benefit an "in-crowd" of academics while at the same time unleveling the academic playing field and increasing inequality.

Taken together, we conclude from the rationale analysis that our participants give more consideration to potential sharing costs than to potential sharing benefits. While the sharing benefits become more prevalent with a collaboration framing, the sharing costs are prominent regardless of the hypothetical scenario. Also, while the multiple-choice data highlight direct cost arguments, the unstructured response also features various indirect costs.

## Discussion and Conclusion

Guided by a voluntary disclosure-inspired discussion on the costs and benefits of open science initiatives in accounting, we conducted a comprehensive survey among accounting researchers to examine their perceptions and attitudes towards open science practices. Our findings suggest an "open science dilemma." While our participants, relative to other social scientists, exhibit greater skepticism about the reproducibility of their influential findings, they also appear more reluctant to adopt research material sharing and pre-registering research designs. These findings are also supported by observational data and might be explained by accounting researchers assessing open science practices to be high cost, low private benefit activities.

If our findings are descriptive: What should we do? We propose a two-pronged approach. First, we should question whether our skepticism towards our work is justified. Is this just accountants' "professional skepticism" at work, or should we really be concerned? To gain clarity, it is essential to take stock. Notably, @CGL_2024 conducted a study on the discontinuity of p-values within empirical accounting research, revealing that this discontinuity exists and is more pronounced than in economics [@BCH_2020] but less so than in political science [@GM_2008a] and sociology [@GM_2008a]. Using a different approach based on a set of replication exercises, @G_2023 finds post-2000 accounting studies to be "very fragile" and links this observation to p-hacking. Both studies, similar to ours, advocate for enhanced sharing of research materials and pre-registration as effective remedies.[^13] Conversely, @LSA_2024 suggests that reproduction studies in accounting are not uncommon and that most these studies corroborate existing findings.

[^13]: In a related study, @BCHH_2024 study randomized controlled trial studies in economics and find that only pre-registrations that include pre-analysis plans show muted p-value discontinuities.

These insights notwithstanding, more systematic evidence seems necessary. A notable effort in this direction is the recent large-scale reproduction attempt of studies published in Management Science [@FGHKO_2023]. This initiative systematically reproduced nearly 500 articles from Management Science both before and after the implementation of a Data and Code Policy in 2019. Remarkably, the results of this study suggest that accounting studies published in Management Science are, if anything, more reproducible than those in related fields such as finance.

It would be insightful to compare the findings for Management Science, which also document an impressive effect of their data and code policy, with data on studies published in the Journal of Accounting Research, again pre and post their data policy. These findings could, in turn, be compared with those for studies published in other leading accounting journals that do not have a dedicated data and code sharing policy (yet). Besides systematic large-scale reproduction studies replication games, potentially embodied in doctoral education, are an alternative means to assess the reproducibility of our work while at the same time helping researchers to grow their network and empirical skill sets. Overall, the academic accounting profession should be well-positioned to verify scientific output. Ultimately, assessing the reproducibility of prior work is an assurance and auditing process, an area we know quite a bit about!

The second element of the two-pronged approach would be to promote research material sharing and pre-registration, potentially alongside other open science initiatives that aim to increase the accessibility and reusability of scientific work. As discussed, the incentive problems in this regard are substantial. What is the private benefit of such practices? Does sharing code and data help building academic careers? While associative evidence from other disciplines suggests that papers sharing their data tend to receive more citations [@PDF_2007], sharing research materials demands both skill and time. Tenure and promotion committees seem to rarely acknowledge its positive impact and in extreme cases, they may even perceive it as a negative signal, assuming it diverts effort away from producing outputs deemed more favorable for tenure.

We contend that this mindset lies at the heart of the problem, and we argue that it is fundamentally misguided. Sharing research materials creates important public goods. Consider a world of scientific computing absent of open-source contributions — it would lack the GNU/Linux operating system and its tools, as well as Python, R, Git, LaTeX, and their diverse package ecosystems. In such a world, scientific progress in fields like econometrics, machine learning, and natural language processing would be significantly hindered, if not entirely unfeasible. Our field thrives on the insights gained from these areas, and thus, we collectively benefit from  open source and open science paradigms in our everyday work.

But can sharing accounting research materials generate similar positive externalities? As our competitive advantage primarily arises from institutional, rather than methodological expertise, we, for example, are well-positioned to curate regulatory and firm-level data. Prominent examples include word lists for sentiment analyses in financial settings [e.g., @LM_2011], topic model-based measures of specific firm risks [e.g., @H_2019], and industry classifications based on financial disclosures [e.g., @HP_2016]. Another promising area where accounting data can have a significant impact is the collection and pre-processing of disclosure data. Web scraping of corporate webpages, for example, can yield valuable and informative data [e.g., @BBB_2024]. Additionally, the imminent electronic filing requirements for sustainability reports offer considerable potential. Although researchers face competition from established commercial data providers in this area, the demand for analyzing unstructured textual data necessitates the creation of research-focused data pipelines and products instead of simply trusting (and paying) commercial data vendors. Furthermore, freely licensable research data can remove a major entry barrier for empirical archival accounting researchers, thereby fostering innovation in the field.

While these flagship projects demonstrate that research material sharing in accounting can also be impactful for other domains, to increase the reproducibility of our work more broadly, we need to bring down the costs of research material sharing. For researchers to share materials effectively, reprodicibility and reusability need to be integrated into project development. Data and code policies of leading journals in economics, finance, and accounting might be promoters. As many researchers affected by these policies will agree, transforming an empirical project from a "well it works" scripting exercise to a well-documented replication package that works across multiple platforms is far from trivial. However, this process becomes significantly easier when an open science-oriented workflow is adopted throughout all stages of project development. Additionally, such a workflow eases collaboration among co-authors and helps kick-starting new projects by reusing materials from prior projects.

So, adopting an open science workflow not only reduces the costs of research material sharing but also creates direct benefits for the researcher. However, in our survey, lack of time emerged as a significant barrier to research material sharing. To address this, PhD level courses in scientific computing and project management seem helpful. While well-funded research institutions might have the opportunity to outsource these skills to research professionals, the vast majority of researchers worldwide must acquire them independently. This is not necessarily disadvantageous, as these skills speed up the development of innovative research projects. Traditionally, many accounting PhD programs relied on informal transmission of scientific computing skills from one PhD generation to the next. It seems that these days are over and that dedicated scientific computing courses should become a central component of scientific education in the quantitative social sciences, comparable to the role of lab training in the natural sciences.

The discussion so far primarily concerns the quantitative empirical fields within our discipline. What about analytical and qualitative researchers and their work? As our findings show, their views on open science diverge significantly from those of the "accounting mainstream." Analytical researchers tend to exhibit greater confidence in their work, possibly in line with theoretical work being easier to verify during the review process. But to achieve the impact it deserves, analytical research must not only be reproducible but also accessible and reusable, ideally even by researchers employing different methodologies. Qualitative work, however, faces conditions that set it apart from typical quantitative work. First, its data are often collected under confidentiality agreements, legally restricting future sharing. Second, its unique richness poses inherent challenges for sharing across subjects, particularly when re-identification issues arise. Third, from a more epistemological perspective, the assumption of objectivity — which permits research materials to be detached from the individual researcher and reused — is scrutinized within the qualitative domain. Consequently, qualitative scholars are developing their own understanding of reproducibility within their methodological paradigm [e.g., @M_2014; @FRV_2022].

Finally, fears and a general lack of trust among researchers significantly hinder the adoption of open science practices. This is human but also unfortunate, as it can lead to subtle "home biases" in sharing, potentially exacerbating academic inequality and hindering scientific progress. The fear of being "scooped" can be mitigated by sharing materials later in the publication process or asserting intellectual ownership through pre-registration and well-established repositories. However, concerns about future researchers identifying flaws and questionable research practices in one's work and a general distrust are more complex challenges to overcome. To address these issues, we must work towards transforming the error climate within the academic discipline. Emphasizing the collaborative and public good nature of open science, rather than a "policing" approach to reproducibility, could be beneficial. By adopting such a climate, we can support each other, advance our field, and make the research process (even) more enjoyable.

\newpage

## References {.unnumbered}

\singlespacing

\small

::: {#refs}
:::

\doublespacing

::: 

::: {.content-visible when-meta="paper-figures"}

[\newpage]{.content-visible when-meta="paper-body"}

## Figures {.unnumbered}

### Figure 1: Sample Comparison between BYU and Our Sample - Subfields {.unnumbered}

```{r Figure1, fig.align='center', fig.width=10, fig.height=6}
byu_fields_comp(byu, sr)
```

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```

This figure compares the distribution of authors' subfields in the BYU sample to the distribution of respondents' self-reported primary subfield in our sample. In contrast to our survey, authors are classified in multiple subfields in the BYU sample. Therefore, this figure compares the 2,877 subfield classifications of the BYU sample to the 229 primary subfield classifications of our sample.

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
```

### Figure 2: Sample Comparison between BYU and Our Sample - Methods {.unnumbered}

```{r Figure2, fig.align='center', fig.width=10, fig.height=6}
byu_methods_comp(byu, sr)
```

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```

This figure compares the distribution of authors' methodologies in the BYU sample to the distribution of respondents' self-reported primary methodology in our sample. In contrast to our survey, authors are classified in multiple methodologies in the BYU sample. Therefore, this figure compares the 2,114 methodology classifications of the BYU sample to 229 primary methodology classifications of our sample.

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
\newpage
```

### Figure 3: Reproducibility of Influential Findings Across the Social Sciences {.unnumbered}

```{r Figure3, fig.align='center', fig.width=9, fig.height=5}
likert_plot(mr, "reprod", "demog_discipline", title_str = "How confident are you that the influential research findings in your area will replicate/reproduce?", llevels = vars$resp_label[80:84])
```

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```

This figure is based on our survey data merged with the data collected by @FLCPSWMBP_2023. The question asked in our survey is "`r vars$text[80]`" while the original question used in the survey of @FLCPSWMBP_2023 is "How confident are you that the influential research findings in \[Discipline\] would replicate?". Our survey and the merging process of the data are discussed in more detail in section 2 of the paper.

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
```

### Figure 4: Importance of Research Material Sharing Across the Social Sciences {.unnumbered}

```{r Figure4, fig.align='center', fig.width=9, fig.height=5}
likert_plot(mr, "sharing_important", "demog_discipline", title_str = "To what extent do you believe that publicly sharing research materials is important?", llevels = vars$resp_label[49:53])
```

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```

This figure is based on our survey data merged with the data collected by @FLCPSWMBP_2023. The question asked in our survey is "`r vars$text[49]`" while the original question used in the survey of @FLCPSWMBP_2023 is "To what extent do you believe that publicly posting (data or code \| research instruments) online is important for progress in \[Discipline\]?". Our survey and the merging process of the data are discussed in more detail in section 2 of the paper.

```{=tex}
\end{singlespace}
\setlength{\parindent}{4em}
\newpage
```

### Figure 5: Sharing of Research Materials: Prescriptive versus Descriptive Views {.unnumbered}

```{r Figure5, fig.align='center', fig.width=8, fig.height=6}
pres_desc_plot(sr)
```

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
``` 

This figure is based on our survey data and summarizes the findings from three separate questions. The first question, related to the preferences of editors is "`r vars$text[79]`". The second question, related to the preferences of other researchers, is "`r vars$text[78]`". The third question, related to the the actual behavior of other researchers, is "`r vars$text[54]`". Our survey is discussed in more detail in section 2 of the paper.

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
\newpage
```

### Figure 6: Pre-Registration: Take-up over Time Across the Social Sciences {.unnumbered}

```{r Figure6, fig.align='center', fig.width=6, fig.height=4}
pre_reg_over_time_plot(mr) + theme(title = element_text(size = 10))
```

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```

This figure is based on our survey data merged with the data collected by @FLCPSWMBP_2023. The question of our survey is "`r vars$text[102]`". The question used in the survey of @FLCPSWMBP_2023 is "Approximately when was the first time you pre-registered hypotheses or analyses in advance of a study?". Our survey and the merging process of the data are discussed in more detail in section 2 of the paper.

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
```

### Figure 7: Share of Papers with Shared Research Materials over Time {.unnumbered}

```{r Figure7, fig.align='center', fig.width=6, fig.height=4}
plot_resmat_sharing_over_cyear
```

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```

This figure is based on `r nrow(papers)` papers, published since 2016 by a country-stratified random sample of `r nrow(authors)` authors from our target participant population. We collect the papers' DOIs from Scopus. For each paper, we observe whether research material availability is mentioned on the paper's DOI URL. The respective shares of papers are plotted by DOI creation year, along with their binomial confidence intervals. DOI creation years with less than 50 papers are excluded.

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
```

::: 

::: {.content-visible when-meta="paper-tables"}

[\newpage]{.content-visible when-meta="paper-figures"}

## Tables {.unnumbered}

### Table 1: Sample Composition {.unnumbered}

```{=tex}
\renewcommand{\thetable}{\arabic{table}}
\setcounter{table}{0}
\begin{singlespace}
\setlength{\parindent}{0em}
```

|                                 |                    N (%) |
|:--------------------------------|-------------------------:|
| Authors identified (Raw Sample) |                    2,660 |
| \- Excluded or missing          | 118 (4.4% of identified) |
| = Emails sent                   |                    2,542 |
| \- Emails bounced               |       206 (8.1% of sent) |
| = Emails received               |                    2,336 |
| Surveys Started                 |      391 (15.4% of sent) |
| Surveys Partially Completed     |      254 (10.9% of sent) |
| Surveys Fully Completed         |       222 (8.7% of sent) |

This table reports on our sample composition. Starting with a list of authors that we identified by Scopus to have published in the "Top 6" accounting journals (Accounting, Organizations and Society, Contemporary Accounting Research, Journal of Accounting and Economics, Journal of Accounting Research, Review of Accounting Studies, The Accounting Review) over the period 2016 to 2021, we excluded all authors with missing email information. We recorded any email bounce messages that we received but report the response rate based on the emails that we sent out. Our survey is discussed in more detail in section 2 of the paper.

```{=tex}
\newpage
```

### Table 2: Sample Demographics {.unnumbered}

#### Panel A: Sample Descriptives {.unnumbered}

```{r Tab2aDemographics, asis = TRUE}
extract_df <- function(dvar, title) {
	df <- create_demog_table(sr, dvar, df_only = TRUE)
	names(df) <- c("Characteristic", "n", "%")
	bind_rows(
		tibble(
			Characteristic = title,
			n = "",
			`%` = ""
		),
		df %>% mutate(Characteristic = paste0("- ", Characteristic))
	)
}

tab <- bind_rows(
	extract_df("demog_field", "Subfield"),
	extract_df("demog_method", "Method"),
	extract_df("demog_age", "Age Group"),
	extract_df("demog_gender", "Gender")
)
gt(tab) %>% as_latex()

```

#### Panel B Regional Representativeness {.unnumbered}

```{r Tab2bGeoRep, asis = TRUE}
tab <- create_regions_rep_table(pop, sr, shorten = TRUE)
tab_latex <- gt(tab) %>% 
	cols_align("right", -1) %>% 
	tab_spanner(label = "Population", columns = starts_with("Population")) %>%
	tab_spanner(label = "Sample", columns = starts_with("Sample")) %>%
	cols_label(
    ends_with(" n") ~ "n",
    ends_with(" %") ~ "%",
    ends_with("Rate") ~ "Regional RR"
	) %>%
		as_latex() 
# tab_latex[1] <- sub("\\\\toprule\\n", "", tab_latex)
tab_latex[1] <- sub("\\{Sample\\} &  ", "{Sample} &  \\\\multirowcell{2}{Regional \\\\\\\\ Response Rate}", tab_latex)
sub("& Regional RR", "& ", tab_latex)
```

This table details the demographics of our sample. As respondents were free not to answer any given question, the total number of answers vary by demographic. In Panel B, we contrast the geographical distribution of our sample with the geographical distribution of the survey population. For this, we use the affiliation of our authors as indicated by Scopus. For `r sum(is.na(pop$affil_country))` cases, the affiliation's country is unavailable on Scopus. Our survey is discussed in more detail in section 2 of the paper.


```{=tex}
\newpage
\begin{landscape}
```

### Table 3: Reproducibility of Influential Findings Across the Social Sciences {.unnumbered}

```{r Tab3ReprodSocSciResp, asis = TRUE}
tab <- create_pct_cross_tab_table(
	mr, "reprod", "demog_discipline",
	resp_label =  get_resp_label("reprod_acc"), df_only = TRUE
)
gt(tab) %>% as_latex()
```

This table is based on our survey data merged with the data collected by @FLCPSWMBP_2023. The question asked in our survey is "`r vars$text[80]`" while the original question used in the survey of @FLCPSWMBP_2023 is "How confident are you that the influential research findings in \[Discipline\] would replicate?". Our survey and the merging process of the data are discussed in more detail in section 2 of the paper. The asterisks report the results of t-tests for mean differences between the respective field and Accounting. (\*/\*\*/\*\*\*) indicate two-sided significance at the (10%/5%/1%) level, respectively.

```{=tex}
\newpage
```

### Table 4: Reproducibility of Influential Findings: Assessment Across Methods {.unnumbered}

#### Panel A: General Assessment by Method {.unnumbered}

```{r Tab4aReprMethod, asis = TRUE}
tab <- create_pct_cross_tab_table(
	sr, "reprod_acc", "demog_method", include_total = TRUE,
	drop_levels = "Other", ctests = TRUE, df_only = TRUE
) 
gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

#### Panel B: Method-Specific Assessment {.unnumbered}

```{r Tab4bReprMethod, asis = TRUE}
tab <- create_pct_cross_tab_table(
	sr, "reprod_method", "demog_method", include_total = TRUE,
	drop_levels = "Other", ctests = TRUE, df_only = TRUE
) 
gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

```{=tex}
\newpage
```

#### Panel C: Assessment Gap (Method - General) {.unnumbered}

```{r Tab4cReprMethod, asis = TRUE}
sr$reprod_method_delta <- as.numeric(sr$reprod_method) - as.numeric(sr$reprod_acc)
tab <- create_pct_cross_tab_table(
	sr, "reprod_method_delta", "demog_method", include_total = TRUE,
	drop_levels = "Other", 
	resp_label = sort(na.omit((unique(sr$reprod_method_delta)))),
	ctests = TRUE, add_num_labels = FALSE, df_only = TRUE 
) 
gt(tab) %>%
	cols_align("right", -1) %>% 
	as_latex()
```

This table is based on our survey data and two separate questions. The response presented in Panel A is based on the question "`r vars$text[80]`" while the response presented in Panel B is based on the question "`r vars$text[90]`". Panel C reports the individual level differences, measured in likert steps, between the answers given for Panel B and Panel A. In all method break-ups, but not for the totals, answers from three respondents who classified their method as "Other" are excluded. Methods where the answers differ at the two-sided 10%-level are indicated by their first two letters.

```{=tex}
\newpage
```

### Table 5: Sharing of Research Materials Across the Social Sciences {.unnumbered}

#### Panel A: Familiarity {.unnumbered}

```{r Tab5aSharingHeardSocSciResp, asis = TRUE}
tab <- create_pct_cross_tab_table(
	mr, "sharing_heard", "demog_discipline", 
	resp_label = c("No (0)", "Yes (1)"), 
	add_num_labels = FALSE, df_only = TRUE
) 
gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

#### Panel B: Assessed Importance {.unnumbered}

```{r Tab5bSharingImportanceSocSciResp, asis = TRUE}
tab <- create_pct_cross_tab_table(
	mr, "sharing_important", "demog_discipline", 
	resp_label = get_resp_label("sharing_important"), df_only = TRUE
) 
gt(tab ) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

This table is based on our survey data merged with the data collected by @FLCPSWMBP_2023. For Panel A, the question of our survey is "`r vars$text[44]`" where "Not at all familiar" is coded as "No". The question used in the survey of @FLCPSWMBP_2023 is "Have you ever heard of the practice of publicly posting (data and code \| study instruments) online for a completed study?". For Panel B, the question of our survey is "`r vars$text[49]`". The question used in the survey of @FLCPSWMBP_2023 is "To what extent do you believe that publicly posting (data or code \| research instruments) online is important for progress in \[Discipline\]?" Our survey and the merging process of the data are discussed in more detail in section 2. The asterisks report the results of t-tests for mean differences between the respective field and Accounting. (\*/\*\*/\*\*\*) indicate two-sided significance at the (10%/5%/1%) level, respectively.

```{=tex}
\newpage
```

### Table 6: Sharing of Research Materials Across Methods {.unnumbered}

#### Panel A: Familiarity {.unnumbered}

```{r Tab6aSharingFamiliarity, asis = TRUE}
tab <- create_pct_cross_tab_table(
	sr, "sharing_familiar", "demog_method", include_total = TRUE,
	drop_levels = "Other", ctests = TRUE, df_only = TRUE
) 
gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

#### Panel B: Assessed Importance {.unnumbered}

```{r Tab6bSharingImportance, asis = TRUE}
tab <- create_pct_cross_tab_table(
	sr, "sharing_important", "demog_method", include_total = TRUE,
	drop_levels = "Other", ctests = TRUE, df_only = TRUE
) 
gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

This table is based on our survey data and two separate questions. The response presented in Panel A is based on the question "`r vars$text[44]`" while the response presented in Panel B is based on the question "`r vars$text[49]`". In all method break-ups, but not for the totals, answers from three respondents who classified their method as "Other", and one observation from a person who chose not self-classify into a main method are excluded. Methods where the answers differ at the two-sided 10%-level are indicated by their first two letters.

```{=tex}
\end{landscape}
\newpage
```

### Table 7: Sharing of Research Materials: Prescriptive versus Descriptive Views {.unnumbered}

#### Panel A: Stated sharing frequency across methods {.unnumbered}

```{r Tab8aSharingUse, asis = TRUE}
tab <- create_pct_cross_tab_table(
	sr, "sharing_use", "demog_method", include_total = TRUE, 
	drop_levels = "Other", ctests = TRUE, df_only = TRUE
) 
gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

#### Panel B: Sharing preference and sharing behavior believes across fields {.unnumbered}

```{r Tab8bSharingPreDes, asis = TRUE}
base_df <- sr %>%
	select(
		demog_method, sharing_perc_other_prefer, 
		sharing_perc_editor_prefer, sharing_perc_other_share
	) %>%
	na.omit()

stats_total <- base_df %>%
	pivot_longer(-1, names_to = "var", values_to = "values") %>%
	group_by(var) %>%
	summarise(
		Mean = sprintf("%.1f%%", mean(values)),
		`Std. Dev.` = sprintf("%.1f%%", sd(values)),
		`25%` = sprintf("%.1f%%", quantile(values, 0.25)),
		Median = sprintf("%.1f%%", median(values)),
		`75%` = sprintf("%.1f%%", quantile(values, 0.75)),
	) %>% 
	pivot_longer(-1, names_to = "stat", values_to = "Total") %>% 
	select(Total)

stats_fields <- base_df %>%
	filter(as.character(demog_method) != "Other") %>%
	group_by(demog_method) %>%
	pivot_longer(-1, names_to = "var", values_to = "values") %>%
	group_by(var, demog_method) %>%
	summarise(
		Mean = sprintf("%.1f%%", mean(values)),
		`Std. Dev.` = sprintf("%.1f%%", sd(values)),
		`25%` = sprintf("%.1f%%", quantile(values, 0.25)),
		Median = sprintf("%.1f%%", median(values)),
		`75%` = sprintf("%.1f%%", quantile(values, 0.75)),
	) %>% 
	pivot_longer(-1:-2, names_to = "stat", values_to = "values") %>%
	pivot_wider(
		id_cols = c(var, stat), 
		names_from = demog_method, values_from = "values"
	) %>% 
	ungroup() %>%
	select(-var) %>%
	rename(" " = stat)

stats <- bind_cols(stats_total, stats_fields) %>%
	select(" ", Total, everything())

tab <- rbind(
	c("General Preference for Sharing", rep("", 6)),
	stats[6:10,],
	c("Editor Preference for Sharing", rep("", 6)),
	stats[1:5,],
	c("Believes About Actual Sharing Behavior", rep("", 6)),
	stats[11:15,]
)

tab_latex <- gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()

tab_latex <- sub("General Preference for Sharing &  &  &  &  &  &", "\\\\multicolumn{7}{l}{General Preference for Sharing}", tab_latex)
tab_latex <- sub("Editor Preference for Sharing &  &  &  &  &  &", "\\\\multicolumn{7}{l}{Editor Preference for Sharing}", tab_latex)
sub("Believes About Actual Sharing Behavior &  &  &  &  &  &", "\\\\multicolumn{7}{l}{Believes About Actual Sharing Behavior}", tab_latex)
```

This table is based on our survey data and several separate questions. The response presented in Panel A is based on the question "`r vars$text[55]`" while the response presented in Panel B is based on the questions "`r vars$text[78]`" (general preference for sharing), "`r vars$text[79]`" (editor preference for sharing) and "`r vars$text[54]`" (actual sharing behavior). In all method break-ups, but not for the totals, answers from three respondents who classified their method as "Other", and one observation from a person who chose not to self-classify into a main method are excluded. Methods where the answers differ at the two-sided 10%-level are indicated by their first two letters. Please see [Online-]{.content-visible when-meta="online-appendix"}Appendix D for an overview of how we adapted these three questions and their answer scales from @FLCPSWMBP_2023.

```{=tex}
\newpage
\begin{landscape}
```

### Table 8: Pre-Registration Across the Social Sciences {.unnumbered}

#### Panel A: Awareness {.unnumbered}

```{r Tab11aPreRegAwareSocSciResp, asis = TRUE}
tab <- create_pct_cross_tab_table(
	mr, "prereg_heard", "demog_discipline", 
	resp_label = c("No (0)", "Yes (1)"), 
	add_num_labels = FALSE, df_only = TRUE
) 
gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

#### Panel B: Trustworthiness {.unnumbered}

```{r Tab11cPreRegTimeSocSciResp, asis = TRUE}
tab <- create_pct_cross_tab_table(
	mr, "prereg_opinion", "demog_discipline", 
	resp_label = get_resp_label("prereg_trust"), df_only = TRUE
) 
gt(tab ) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

This table is based on our survey data merged with the data collected by @FLCPSWMBP_2023. For Panel A, the question of our survey is "`r vars$text[95]`" where we code the response "Not at all familiar" as "No" and all other responses as "Yes." The question used in the survey of @FLCPSWMBP_2023 is "Have you ever heard of the practice of pre-registering hypotheses or analyses in advance of a study?". For Panel B, the question of our survey is "`r vars$text[103]`". The question used in the survey of @FLCPSWMBP_2023 is "What is your opinion of pre-registering hypotheses or analyses?". Our survey and the merging process of the data are discussed in more detail in section 2 of the paper. The asterisks report the results of t-tests for mean differences between the respective field and Accounting. (\*/\*\*/\*\*\*) indicate two-sided significance at the (10%/5%/1%) level, respectively.

```{=tex}
\newpage
```


### Table 9: Pre-Registration Across Methods {.unnumbered}

#### Panel A: Familiarity {.unnumbered}

```{r Tab12aPreRegFamiliarity, asis = TRUE}
tab <- create_pct_cross_tab_table(
	sr, "prereg_familiar", "demog_method", include_total = TRUE,
	drop_levels = "Other", ctests = TRUE, df_only = TRUE
) 
gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

#### Panel B: Usage {.unnumbered}

```{r Tab12bPreRegUse, asis = TRUE}

tab <- create_pct_cross_tab_table(
	sr, "prereg_use", "demog_method", include_total = TRUE,
	drop_levels = "Other", 
	resp_label = c("No (0)", "Yes (1)"), 
	add_num_labels = FALSE, ctests = TRUE, df_only = TRUE
) 
# Adjust levels to be zero based.
tab[4,2:7] <- t(sprintf("%.2f", as.numeric(tab[4,2:7]) - 1))
tab[5,2:7] <- t(sprintf("%.2f", as.numeric(tab[5,2:7]) - 1))
gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

```{=tex}
\newpage
```

#### Panel C: Trustworthiness {.unnumbered}

```{r Tab12cPreRegTrust, asis = TRUE}

tab <- create_pct_cross_tab_table(
	sr, "prereg_trust", "demog_method", include_total = TRUE,
	drop_levels = "Other", ctests = TRUE, df_only = TRUE
) 
gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

This table is based on our survey data and three separate questions. The response presented in Panel A is based on the question "`r vars$text[95]`", the response presented in Panel B is based on the question "`r vars$text[100]`", and the response presented in Panel C is based on the question "`r vars$text[103]`". In all method break-ups, but not for the totals, answers from three respondents who classified their method as "Other" are excluded. Methods where the answers differ at the two-sided 10%-level are indicated by their first two letters.

```{=tex}
\end{landscape}
\newpage
```


### Table 10: Observational Data on Research Material Sharing {.unnumbered}

#### Panel A: By Author {.unnumbered}

```{r Tab9aObservationalDataByAuthor, asis = TRUE}
gt(tab_by_author) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

#### Panel B: By Journal {.unnumbered}

```{r Tab9bObservationalDataByJournal, asis = TRUE}
gt(tab_by_journal) %>% 
	cols_align("right", -1) %>% 
	as_latex()

```

This table is based on on a country-stratified random sample of `r nrow(authors)` authors from our target participant population and `r nrow(papers)` papers that they published since 2016. We collect these publications' DOIs using Scopus. To identify research material sharing, we search homepages and the established open science platforms GitHub, Open Science Foundation (OSF), Dataverse, and Zenodo. For each paper, we observe whether research material availability is mentioned on the paper's DOI URL. The data collection manual for this process is available in this project's [[GitHub repository](https://github.com/joachim-gassen/osacc/)]{.content-visible unless-meta="anonymize"}[GitHub repository (link omitted for review)]{.content-visible when-meta="anonymize"}. In Panel A, the overall share of authors that we identify to share research materials is reported, along with its binomial confidence interval in brackets. We also report the share of authors that we document to provide research materials when we ignore the Journal of Accounting Research and Management Science, as both journals have mandatory data and code sharing policies in place. In addition, we report the share of authors that have accounts linked to their names on the open science platforms mentioned above. Panel B reports the paper-level data in total and separately for all journals for which we evaluated at least 20 papers.

```{=tex}
\newpage
```



### Table 11: Scenario Analysis: Hypothetical Sharing Likelihood {.unnumbered}

#### Panel A: Descriptive Statistics {.unnumbered}

```{r Tab7aExpDescriptives, asis = TRUE}
stats <- sr %>%
	group_by(exp_tment_control, exp_tment_private) %>%
	select(exp_tment_control, exp_tment_private,exp_pct_share) %>%
	summarise(
		N = sprintf("%d", n()),
		Mean = sprintf("%.1f%%", mean(exp_pct_share)),
		`S.D.` = sprintf("%.1f%%", sd(exp_pct_share)),
		Min = sprintf("%.1f%%", min(exp_pct_share)),
		`25%` = sprintf("%.1f%%", quantile(exp_pct_share, 0.25)),
		Median = sprintf("%.1f%%", median(exp_pct_share)),
		`75%` = sprintf("%.1f%%", quantile(exp_pct_share, 0.75)),
		Max = sprintf("%.1f%%", max(exp_pct_share))
	) %>%
	ungroup() %>%
	mutate(
		Rationale = c(rep("Collaboration", 2), rep("Control", 2)),
		Scope = c(rep(c("Public", "Private"), 2))
	) %>%
	select(-exp_tment_control, -exp_tment_private) %>%
	select(Rationale, Scope, everything())
tab <- create_pct_cross_tab_table(
	sr, "reprod_acc", "demog_method", 
	ctests = TRUE, df_only = TRUE
) 
gt(stats) %>% 
	cols_align("right", -1:-2) %>% 
	as_latex()
```

#### Panel B: Regression Results {.unnumbered}

```{r Tab7bExpRegResults, asis = TRUE}
tab_latex  <- modelsummary(
	list(H1 = modh1, H2 = modh2, H3 = modh3), output = "gt",
	coef_rename = function(x) {
		x <- str_replace(x, fixed("exp_tment_collab"), "Collaboration rationale")
		x <- str_replace(x, fixed("exp_tment_private"), "Private scope")
		str_replace(x, fixed(":"), "×")
	},
	gof_map = list(
		list(raw = "adj.r.squared", clean = "Adjusted R2", fmt = 3),
		list(
			raw = "nobs", clean = "Number of observations", 
			fmt = function(x) format(x, big.mark=",")
		)
	),
	estimate  = "{estimate}",
	statistic = c("conf.int", "({p.value})")
) %>% as_latex()

# The below is needed since some tex environment chock on the longtable environment
# See https://github.com/vincentarelbundock/modelsummary/issues/50
tab_latex[1] <- sub("\\(Intercept\\)", "Intercept", tab_latex) 

sub("Adjusted R2", "\\\\midrule\\\\addlinespace[2.5pt]\nAdjusted R²", tab_latex)
```

This table reports the results from the pre-registered experimental design contained in our survey. Is is based on a hypothetical code and data sharing scenario were we manipulated the sharing rationale (collaboration versus control) and the sharing scope (private versus public). All experimental materials are detailed in the [Online-]{.content-visible when-meta="online-appendix"}Appendix of this study. The outcome variable is the stated likelihood that the respondent would share their code and data under the hypothetical scenario. Panel A reports the descriptive statistics for this outcome, conditional for the different treatment cells. Panel B reports the results from OLS regressions of the outcome on the binary treatments (H1 and H2) and their interaction (H3). 95% confidence intervals and two-sided t-test based p-values in parentheses are reported below the coefficients.

```{=tex}
\newpage
```


### Table 12: Sharing of Research Materials: Rationales {.unnumbered}

#### Panel A: Reasons against Sharing (Close-Ended Question) {.unnumbered}

```{r Tab10aSharingReasons, asis = TRUE}
tab <- sr %>%
	select(starts_with("sharing_reason"), -starts_with("sharing_reason_other")) %>% 
	na.omit() %>%
	pivot_longer(everything(), names_to = "reason", values_to = "val") %>%
	group_by(reason) %>%
	summarise(
		sum = sum(val),
		Total = sprintf("%d (%.1f %%)", sum(val), 100*sum(val)/n())
	) %>%
	arrange(-sum) %>% 
	select(-sum) %>%
	left_join(
		sr %>%
			select(demog_method, starts_with("sharing_reason"), -starts_with("sharing_reason_other")) %>% 
			filter(demog_method != "Other") %>%
			na.omit() %>%
			pivot_longer(-1, names_to = "reason", values_to = "val") %>%
			group_by(demog_method, reason) %>%
			filter(sum(val) != 0) %>%
			summarise(
				stat = sprintf("%d (%.1f %%)", sum(val), 100*sum(val)/n())
			) %>%
			pivot_wider(id_cols = reason, names_from = demog_method, values_from = stat), 
		by="reason"
	) %>%
	mutate(
		reason =str_remove(reason, fixed("sharing_reason_")) %>% 
				str_replace_all(fixed("_"), " ") %>% str_to_sentence()
		) %>%
	rename(Reason = reason)

gt(tab) %>% 
	cols_align("right", -1) %>% 
	sub_missing(missing_text = "") %>%
	as_latex()
```

```{=tex}
\newpage
\begin{landscape}
```

#### Panel B: Rationales about Sharing (Open-Ended based on Hypothetical Scenario) {.unnumbered}

```{r Tab10bSharingRationales, asis = TRUE}
df <- svr %>% 
	mutate(
	verbal_gave_reasons = !is.na(verbal_n_reasons),
	verbal_mostly_positive = 1*(verbal_positive_reasons > verbal_negative_reasons),
	exp_tment_colab = 1 - exp_tment_control,
	exp_tment_group = 2*exp_tment_colab + exp_tment_private,
	demog_active_res = as.numeric(demog_active_res) - 1,
	demog_trust = (demog_trusting + demog_trustworthy)/2
)

reasons_overall <- df %>%
	filter(verbal_gave_reasons) %>%
	summarise(across(
		starts_with(c("verbal_pos_", "verbal_neg_", "verbal_dep_")), sum
	)) %>%
	pivot_longer(
		starts_with(c("verbal_pos_", "verbal_neg_", "verbal_dep_")), values_to = "n", names_to = "reason"
	) %>% mutate(
		reason = str_remove(reason, fixed("verbal_")),
		pct_all = n/sum(df$verbal_gave_reasons) 		
	)

reasons_by_tgroup <- df %>%
	filter(verbal_gave_reasons) %>%
	group_by(exp_tment_group) %>%
	summarise(across(
		starts_with(c("verbal_pos_", "verbal_neg_", "verbal_dep_")), 
		~ {sum(.x)/n()} 
	)) %>%
	pivot_longer(
		starts_with(c("verbal_pos_", "verbal_neg_", "verbal_dep_")), values_to = "pct", names_to = "reason"
	) %>% pivot_wider(
		names_from = "exp_tment_group", names_prefix = "pct_tg", values_from = "pct"
	) %>%
	mutate(
		reason = str_remove(reason, fixed("verbal_"))
	)

reasons <- reasons_overall %>% left_join(reasons_by_tgroup, by = "reason")

tab <- reasons %>%
	mutate(
		type = str_extract(reason, "pos|neg|dep"),
		reason = paste(
			"...",
			str_remove(reason, "pos_|neg_|dep_") %>% 
				str_replace_all(fixed("_"), " ") %>%
				tolower()
		),
		across(starts_with("pct"), ~ sprintf("%.1f %%", 100 * .x))
	) %>%
	mutate(other = (reason == "... other reasons")) %>%
	arrange(type, other, -n) %>%
	select(type, everything()) %>%
	select(-other)

names(tab) <- c(
	"Rationale", "Reason", "N", 
	"% Total", "% Control/Public", "% Control/Private", 
	"% Collab/Public", "% Collab/Private"
)

tab <- rbind(
	c("I share because it ... ", rep("", 6)),
	tab %>% filter(Rationale == "pos") %>% select(-Rationale),
	c("I do not share because ... ", rep("", 6)),
	tab %>% filter(Rationale == "neg") %>% select(-Rationale),
	c("It depends ... ", rep("", 6)),
	tab %>% filter(Rationale == "dep") %>% select(-Rationale)
)

tab_latex <- gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()

sub(
	"career concerns related to failed replications or corrections",
	"career concerns related to failed replications", tab_latex
)
```

This table is based on our survey data and two separate questions. The response presented in Panel A is based on the multiple choice answer to the close-ended question "`r vars$text[60]`". Participants answered this question if they expressed not always sharing their research materials. Panel B is based on the answer to the open-ended question that we pose as a follow-up in our experimental scenario: "`r vars$text[7]`". The free form answers to this question have been coded by an research assistant that had no information about the experimental design or the respective treatments. In Panel A, answers from two respondents who classified their method as "Other" are included in the Total column, but excluded from the methods break-up.

```{=tex}
\end{landscape}
\end{singlespace}
```

::: 

::: {.content-visible when-meta="appendix"}

[\newpage]{.content-visible when-meta="paper-tables"}

## [Online-]{.content-visible when-meta="online-appendix"}Appendix {.unnumbered}

::: {.content-visible unless-meta="anonymize"}

The public GitHub repository at [https://github.com/joachim-gassen/osacc](https://github.com/joachim-gassen/osacc) contains the data and the code of this project. To evaluate our survey data in more depth, we encourage the use of our [online survey dashboard](https://jgassen.shinyapps.io/osacc/). Using this dashboard, you can explore the response to all survey questions, partitioning it by respondents' demographics.

:::

## [Online-]{.content-visible when-meta="online-appendix"}Appendix A: Survey Administration and Instrument {.unnumbered}

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
```

### Initial Invitation - November 21, 2022 {.unnumbered}

\[Title: Share your views on Accounting Research\]

Dear colleague,

We are conducting a study about sharing research materials, reproducibility, and pre-registration in accounting research. You have been chosen to provide your opinion on this subject because you are a published author in one of the leading accounting journals. We obtained your email address from publicly available sources.

Participating in our study takes around 7 minutes of your time. We will publicly share the anonymized data and results with the academic community in due course.

As a token of gratitude for your time, we will donate 1,000 € multiplied by the response rate to the United Nations International Children's Emergency Fund (UNICEF).

Click or copy and paste the URL below in your internet browser to participate in our study:\
\[URL\]

Should you have any other comments or questions, please feel free to contact us via \[EMAIL\].

We thank you for your consideration and participation.

Sincerely,

[[omitted for review]]{.content-visible when-meta="anonymize"}

::: {.content-visible unless-meta="anonymize"}

Joachim Gassen\
Humboldt-Universität zu Berlin\
TRR 266 Accounting for Transparency

Jacqueline Klug, Victor van Pelt\
WHU -- Otto Beisheim School of Management

:::

Click here to unsubscribe from future e-mails. Click here to view our data protection guidelines.

Further information:

This study has been approved by the ethical review board of [the WHU -- Otto Beisheim School of Management]{.content-visible unless-meta="anonymize"}[[omitted for review]]{.content-visible when-meta="anonymize"}.

Participation in our study is entirely voluntary, and there are no consequences for you if you choose not to take part in it. We will ask for your consent to participate at the beginning of the study.

The data will only be analyzed and stored in anonymous form. This anonymous data will only be used for academic purposes and will be shared with other academics for research purposes.

```{=tex}
\newpage
```


### Reminder 1 - November 28, 2022 {.unnumbered}

\[Title: Reminder: Share your views on Accounting Research\]

Dear colleague,

If you have not completed our **study** about **sharing research materials, reproducibility, and pre-registration in accounting research** yet, you still have time!

Participate in our **7-minute** study by following the URL below:

\[URL\]

As a token of gratitude for your time, we will **donate 1,000 € multiplied by the response rate** to the United Nations International Children's Emergency Fund (**UNICEF**).

Should you have any other comments or questions, please feel free to contact us via \[EMAIL\].

We thank you for your consideration and participation.

Sincerely,

[[omitted for review]]{.content-visible when-meta="anonymize"}

::: {.content-visible unless-meta="anonymize"}

Joachim Gassen\
Humboldt-Universität zu Berlin\
TRR 266 Accounting for Transparency

Jacqueline Klug, Victor van Pelt\
WHU -- Otto Beisheim School of Management

:::

Click here to unsubscribe from future e-mails. Click here to view our data protection guidelines.

Further information:

This study has been approved by the ethical review board of [the WHU -- Otto Beisheim School of Management]{.content-visible unless-meta="anonymize"}[[omitted for review]]{.content-visible when-meta="anonymize"}.

Participation in our study is entirely voluntary, and there are no consequences for you if you choose not to take part in it. We will ask for your consent to participate at the beginning of the study.

The data will only be analyzed and stored in anonymous form. This anonymous data will only be used for academic purposes and will be shared with other academics for research purposes.

```{=tex}
\newpage
```

### Reminder 2 - December 5, 2022 {.unnumbered}

\[Title: Reminder: Share your views on Accounting Research\]

Dear colleague,

If you have not completed our **study** about **sharing research materials, reproducibility, and pre-registration in accounting research** yet, you still have time **until next Monday**. We will close the study after that date.

Participate in our **7-minute** study by following the URL below:

\[URL\]

As a token of gratitude for your time, we will **donate 1,000 € multiplied by the response rate** to the United Nations International Children's Emergency Fund (**UNICEF**).

Should you have any other comments or questions, please feel free to contact us via \[EMAIL\]

We thank you for your consideration and participation.

Sincerely,

[[omitted for review]]{.content-visible when-meta="anonymize"}

::: {.content-visible unless-meta="anonymize"}

Joachim Gassen\
Humboldt-Universität zu Berlin\
TRR 266 Accounting for Transparency

Jacqueline Klug, Victor van Pelt\
WHU -- Otto Beisheim School of Management

:::

Click here to unsubscribe from future e-mails. Click here to view our data protection guidelines.

Further information:

This study has been approved by the ethical review board of [the WHU -- Otto Beisheim School of Management]{.content-visible unless-meta="anonymize"}[[omitted for review]]{.content-visible when-meta="anonymize"}.

Participation in our study is entirely voluntary, and there are no consequences for you if you choose not to take part in it. We will ask for your consent to participate at the beginning of the study.

The data will only be analyzed and stored in anonymous form. This anonymous data will only be used for academic purposes and will be shared with other academics for research purposes.

```{=tex}
\newpage
```

### Reminder 3 - December 12, 2022 {.unnumbered}

\[Title: Last Chance to Participate: Share your views on Accounting Research\]

Dear colleague,

If you have not completed our **study** about **sharing research materials, reproducibility, and pre-registration in accounting research** yet, this **Monday** is your **last chance to participate!** We will close the study by the end of today.

Participate in our **7-minute** study by following the URL below:

\[URL\]

As a token of gratitude for your time, we will **donate 1,000 € multiplied by the response rate** to the United Nations International Children's Emergency Fund (**UNICEF**).

Should you have any other comments or questions, please feel free to contact us via \[EMAIL\].

We thank you for your consideration and participation.

Sincerely,

[[omitted for review]]{.content-visible when-meta="anonymize"}

::: {.content-visible unless-meta="anonymize"}

Joachim Gassen\
Humboldt-Universität zu Berlin\
TRR 266 Accounting for Transparency

Jacqueline Klug, Victor van Pelt\
WHU -- Otto Beisheim School of Management

:::

Click here to unsubscribe from future e-mails. Click here to view our data protection guidelines.

Further information:

This study has been approved by the ethical review board of [the WHU -- Otto Beisheim School of Management]{.content-visible unless-meta="anonymize"}[[omitted for review]]{.content-visible when-meta="anonymize"}.

Participation in our study is entirely voluntary, and there are no consequences for you if you choose not to take part in it. We will ask for your consent to participate at the beginning of the study.

The data will only be analyzed and stored in anonymous form. This anonymous data will only be used for academic purposes and will be shared with other academics for research purposes.

```{=tex}
\newpage

\includepdf[scale=0.7, pages=1,pagecommand={\hypertarget{survey-instrument}{\subsection*{Survey Instrument}\label{survey-instrument}}\addcontentsline{toc}{subsubsection}{Survey Instrument}}]{materials/survey.pdf}
\includepdf[scale=0.7, pages=2-, pagecommand={}]{materials/survey.pdf}
```

## [Online-]{.content-visible when-meta="online-appendix"}Appendix B: Scenario Manipulations and Instrument {.unnumbered}

### Manipulations {.unnumbered}


::: {#table-stretch}
\renewcommand{\arraystretch}{2} 
:::

: {tbl-colwidths="\[20,40,40\]"}

+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Rationale     | Private Request                                                                                                                                                                                                         | Public Request                                                                                                                                                                                                                                   |
+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Control       | You conducted a study and published it in an academic journal. Another researcher contacts you and asks for access to your research materials (i.e., the data, code, and research procedures). The researcher wants to: | You conducted a study and published it in an academic journal. Another researcher contacts you and asks you to publicly share your research materials (i.e., the data, code, and research procedures). The researcher wants to enable others to: |
|               |                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                  |
|               | -   verify how your study was conducted,                                                                                                                                                                                | -   verify how your study was conducted,                                                                                                                                                                                                         |
|               |                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                  |
|               | -   investigate whether your data and methodology are sound,                                                                                                                                                            | -   investigate whether your data and methodology are sound,                                                                                                                                                                                     |
|               |                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                  |
|               | -   control whether your findings can be reproduced, and                                                                                                                                                                | -   control whether your findings can be reproduced, and                                                                                                                                                                                         |
|               |                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                  |
|               | -   potentially conduct a new study that corrects your study.                                                                                                                                                           | -   potentially conduct a new study that corrects your study.                                                                                                                                                                                    |
+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Collaboration | You conducted a study and published it in an academic journal. Another researcher contacts you and asks for access to your research materials (i.e., the data, code, and research procedures). The researcher wants to: | You conducted a study and published it in an academic journal. Another researcher contacts you and asks you to publicly share your research materials (i.e., the data, code, and research procedures). The researcher wants to enable others to: |
|               |                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                  |
|               | -   understand how your study was conducted,                                                                                                                                                                            | -   understand how your study was conducted,                                                                                                                                                                                                     |
|               |                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                  |
|               | -   learn from your data and methodology,                                                                                                                                                                               | -   learn from your data and methodology,                                                                                                                                                                                                        |
|               |                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                  |
|               | -   contribute by generating new insights, and                                                                                                                                                                          | -   contribute by generating new insights, and                                                                                                                                                                                                   |
|               |                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                  |
|               | -   potentially conduct a new study that builds on your study.                                                                                                                                                          | -   potentially conduct a new study that builds on your study.                                                                                                                                                                                   |
+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+




```{=tex}
\end{singlespace}
```

```{=tex}
\includepdf[scale=0.7, pages=1,pagecommand={\hypertarget{scenario-instrument}{\subsection*{Instrument}\label{scenario-instrument}}\addcontentsline{toc}{subsubsection}{Scenario Instrument}}]{materials/scenario.pdf} \includepdf[scale=0.7, pages=2-, pagecommand={}]{materials/scenario.pdf}
```

## [Online-]{.content-visible when-meta="online-appendix"}Appendix C: Power Analysis {.unnumbered}

```{r AppCFigure1, fig.width=12, fig.height=9}
power_res <- readRDS("../data/static/power_sim_10000_results.rds")
post_hoc_power_figure(power_res)
```


```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```

This figure reports the results of a post hoc power analysis for our experimental findings. To infer the power of our experimental setting and its minimum detectable effect sizes conditional on various response rates, we calibrate a simulated dependent variable on distributional moments that are close to our observed sharing probability (Mean = 60%, Standard Deviation = 25%). To reflect that our outcome measure is naturally bounded and that participants are likely to give round estimates, we round the resulting simulated data to 5% intervals between 0% and 100%. We model our treatment effects for H1 and H2 in percentage points (PP) to be identical (depicted on the horizontal axis) and assume the interaction treatment effect of H3 to be 50% of the main treatment effects. The data presented below are based on 10,000 simulation runs. The vertical axis presents power, measured as the percentage share of collaboration treatment effect 95% confidence intervals that are strictly larger than zero. The family of curves reflects treatment cell sizes ranking from 40 (right-most curve, representing a response rate of \~6.3%) to 100 (left-most curve, representing a response rate of \~15.7%), where response rates are relative to the emails sent. The minimum detectable effect sizes for the various sample sizes can be inferred at the horizontal red dashed 80% power line. They rank from \~10 PP (100 observations per treatment cell) to \~15 PP (40 observations per treatment cell). The black curve reflects a treatment cell size of 60 observations, which is close to our sample size. For this, the minimum detectable effect size is \~12 PP.

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
\newpage
\begin{landscape}
\begin{singlespace}
\begin{center}
```

## [Online-]{.content-visible when-meta="online-appendix"}Appendix D: Comparison of our Survey with Social Science Survey (S3) {.unnumbered}

```{r AppDTab1, asis = TRUE}
df <- read_csv(
	"../data/external/s3_osacc_item_comparison.csv", 
	show_col_types = FALSE
)
gt(df[,1:7]) %>% 
	sub_missing() %>% 
	cols_width(
		`S3 Question` ~ pct(15),
		`Scale S3` ~ pct(15),
		`Our Question` ~ pct(15),
		`Our Scale` ~ pct(15),
		`Recoding` ~ pct(15)
	) %>%
	as_latex() %>%
	gt_add_header_mpage_table()
```

```{=tex}
\end{center}
\end{singlespace}
\end{landscape}
```

::: 
