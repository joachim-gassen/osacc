---
title: |
  | Accountants' Attitudes Towards Open Science^[Joachim Gassen is based at Humboldt-Universität zu Berlin while Jacqueline Klug and Victor van Pelt are affiliated with WHU Otto Beisheim School of Management. We thank the audience of a workshop at Humboldt University for providing helpful feedback. Excellent research assistance by Benedikt Hahn is gratefully acknowledged. This work was supported by Deutsche Forschungsgemeinschaft (Project-ID 403041268, TRR 266 Accounting for Transparency).]
author: 
  - name: Joachim Gassen
    email: gassen@wiwi.hu-berlin.de
    affiliation: Humboldt-Universität zu Berlin
  - name: Jacqueline Klug
    email: Jacqueline.Klug@whu.edu
    affiliation: WHU Otto Beisheim School of Management
  - name: Victor van Pelt
    email: Victor.vanPelt@whu.edu 
    affiliation: WHU Otto Beisheim School of Management
date: last-modified
date-format: long
execute:
  echo: false
  message: false
  warning: false
format:
  pdf:
    documentclass: article
    number-sections: true
    colorlinks: true
    papersize: letter
    fig-pos: H
    fig_caption: yes
    fig-cap-location: top
    geometry: margin=1in
    fontsize: 11pt
    ident: yes

editor: visual

bibliography: references.bib
biblio-style: apsr

always_allow_html: yes

header-includes:
  - \usepackage{setspace}\doublespacing
  - \setlength{\parindent}{4em}
  - \setlength{\parskip}{0em}
  - \setlength{\skip\footins}{2em}
  - \usepackage[hang]{footmisc}
  - \setlength{\footnotemargin}{1em}
  - \usepackage{pdfpages}
  - \usepackage{pdflscape}
  - \usepackage{csquotes}
  - \renewcommand{\mkbegdispquote}[2]{\itshape}
---

```{r setup, include=FALSE}
library(knitr)
library(rmarkdown)
library(gt)
library(modelsummary)
# opts_chunk$set(fig.pos = 'p') # Places figures on their own pages
opts_chunk$set(out.width = '80%', dpi=600)
opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, cache = FALSE)

source("../code/table_functions.R")
source("../code/figure_functions.R")

sd <- readRDS("../data/generated/survey_response.rds")
mr <- readRDS("../data/generated/merged_response.rds")
vars <- read_csv("../data/external/variables.csv", show_col_types = FALSE)
pop <- read_csv("../data/external/invited_authors.csv", show_col_types = FALSE)
```

```{=tex}
\thispagestyle{empty}
\begin{abstract}
\singlespace
Surveying a sample of scholars that have published in leading accounting journals, we document that we accounting researchers are more skeptical about the reproducibility of our influential findings than researchers from other social science areas are about theirs. Also, we are less familiar with public sharing of research materials (data, code, and research procedures) and have less experience with pre-registrating our study designs, two prominent open science measures to increase the reproducibility of research. Our respondents state that they are open to sharing and would be likely to share research materials under a hypothetical scenario, regardless of the experimental framing. They also believe that other accounting researchers, as well as editors, are in favor of research material sharing. However, they are skeptical about the actual prevalence of research material sharing in the accounting research community. In addition, they are more reluctant than researchers from neighboring fields to pre-register their study plans. We interpret this descriptive evidence as indicating an "accounting open science dilemma" and suggest strategies for addressing it.
\end{abstract}
```
\vspace{2cm}

```{=tex}
\begin{flushleft}
\textbf{Keywords}: Accounting Research, Open Science, Reproducibility, Pre-Registration, Survey \newline
\textbf{Data Availability}: Data available in code repository \newline
\textbf{Code Repository}: \href{https://github.com/joachim-gassen/osacc}{github.com/joachim-gassen/osacc}  \newline
\textbf{Declaration of Interest}: The author(s) declare(s) that they have no conflict of interest \newline
\end{flushleft}
```
\pagebreak

\setcounter{page}{1}

## Introduction

Cumulative science builds on reproducible research.[^1] Across all fields of science [@B_2016; @CDF_2016; @M_2017] and within accounting [@HLL_2000; @G_2023; @S_2023; @CGL_2023], scholars have diagnosed shortcomings in reproducibility and advocated open science methods like public sharing of research materials and the pre-registration of study plans as potential strategies to improve the reproducibility of our work. Prior work across the social sciences [@FLCPSWMBP_2023] has documented an overall uptake of open sciences approaches across several disciplines. We contrast these findings with a survey that we conducted among accounting researchers to explore the perceptions and attitudes toward open science in our field.

[^1]: While in the survey itself, we use the term reproducibility without an exact definition, throughout this article, we follow the notion of @NSF_2015 to define reproducibility as the "ability of a researcher to duplicate the results of a prior study using the same materials and procedures as were used by the original investigator" while replicability "refers to the ability of a researcher to duplicate the results of a prior study if the same procedures are followed but new data are collected". For a more in-depth discussion of the related terminological debate, see @GFI_2016. The key reason why we do not provide an exact definition in the survey is that the differences between reproducibility and replicability are subtle and vary across accounting subfields: For archival studies, reproducibility and replicability should be essentially the same, while for experimental studies they clearly differ.

Based on the responses of `r nrow(sd)` published accounting researchers, we find that our attitudes towards open science clearly differ from those of other researchers in the social sciences. Most prominently, we tend to hold a generally more critical view of the reproducibility of our influential work. Compared to the fields of economics and sociology, for example, we are about half a point more skeptical about the reproducibility of the influential work in our field on a 5-point Likert scale. This is a sizable difference.

Exploring this assessment across our respondents' demographics, we find it to be relatively stable. Analytical and qualitative researchers are somewhat more pessimistic than the other methodologies. Analytical researchers tend to be particularly more critical of research that uses non-analytical methods.

Given this finding, one would expect accounting researchers to take a proactive stand and embrace two prominent open science approaches that methodological work has identified to increase the reproducibility of research: the public sharing of research materials (i.e., data, code and research procedures) and the pre-registration of research plans.

Turning first to the public sharing of research materials, we document that, again compared with researchers from other social science areas, we accounting researchers are somewhat less familiar with this activity. Also, we find it somewhat less important. Familiarity is generally largest for respondents using archival, field, and experimental methods and smallest for respondents using qualitative methods, which also perceive sharing to be less important.

Similar to, albeit even more pronounced than, other social science researchers, there is a stark contrast in our perceived (prescriptive) importance of research material sharing, our stated willingness to share research materials, and our (descriptive) perception about actual sharing behavior. In a hypothetical scenario, the average stated likelihood for sharing research materials by our respondents is relatively high (i.e., 72%), and this likelihood is more or less unaffected by the randomized nature of the rationale (i.e., collaboration or control) and the sharing scope (public or private).[^2] Also, 50% of our respondents state that they at least sometimes publicly share their research materials. In addition, we think that about 44% of us favor publicly sharing research materials, and we assess 61% of the top-six accounting editors to be in favor of publicly sharing research materials.

[^2]: We pre-registered this experimental part of our study. The pre-registration materials can be accessed [here](https://osf.io/frmzt/).

All this sounds comforting and consistent with a general supportive stand of the accounting community towards public research material sharing. However, when questioned about the actual percentage of us sharing research materials publicly, the number comes down to a much more sobering 28%. This number and the prescriptive/descriptive gap that it reveals differ significantly across methods: Analytical researchers have the narrowest gap (43% of researchers expected to be in favor and 38% expected to actually share) and archival researchers the widest (45% versus 24%). The overall lowest numbers are again reported by qualitative researchers (36% versus 18%).

In light of these numbers, it is a logical next step to explore the main reasons against sharing research materials. Overall, the most commonly stated reasons are the proprietary nature of used data (56% of respondents give this reason), the time that it takes to make materials available (30%) and the fear of being scooped when materials become publicly available (23%). Also, 34% of respondents state there is no need for sharing as their data are already publicly available. Unsurprisingly, the reasons clearly differ across methods. For qualitative researchers, deidentification problems are very prominent (46%) and analytical researchers consider the time that it takes to make materials available far less of a reason than researchers using the other methods (i.e., 9.5% versus 30% overall).

Turning to pre-registration, it seems that we accounting researchers are at least as familiar with the concept as other social science researchers. However, we started using it later (roughly 2 years) and use it less frequently (17% versus 27%). Our trust in pre-registered work is generally comparable with other social scientists but significantly lower than trust levels of researchers in Psychology (the field that reports the highest usage rates of pre-registration). A detailed look into differences among the different methodological fields within accounting also reflects this as we find that experimental researchers use pre-registration most (28%) and express the strongest belief that the practice increases the trustworthiness of research.


Taken together, our findings indicate somewhat of an "accounting open science dilemma": We accounting researchers tend to be skeptical about both, the reproducibility of our work, and the ability of research material sharing and pre-registration to improve it. This results in these tools currently being less established in the accounting domain than in other fields. In addition, the challenges that hinder reproducibility and the potential effectiveness of open science tools vary across methodological paradigms. Qualitative research faces different challenges than archival quantitative work, and its differ again from experimental studies. We conclude by discussing these challenges and providing suggestions on how our diagnosed dilemma might be overcome.

## Survey Methodology and Variable Measurement

### Survey Design and Administration

To collect the attitudes of accounting researchers towards open science, we conducted a survey among members of the academic accounting community. The survey has been designed using oTree [@CSW_2016]. To recruit respondents for our survey, we identified authors who published in the "Top 6" accounting journals, i.e., Accounting, Organizations and Society, Contemporary Accounting Research, Journal of Accounting and Economics, Journal of Accounting Research, Review of Accounting Studies, The Accounting Review, over the period 2016 to 2021. We chose these six accounting journals because they are commonly known to be top accounting journals within the academic accounting community. We realize that this is not an exhaustive list and that there are divergent views about journals within the accounting community. Yet, we believe that, taken together, these journals provide us with a somewhat representative list of publishing accounting academics. We started compiling the list of authors from 2016 onward because this is the year that Open Science and, more specifically, the reproducibility of research became a more widespread topic of debate among academics within the social sciences and, presumably, accounting researchers [@OSC_2015; @B_2016; @CDF_2016; @M_2017]. After receiving approval from an ethical review board, also guaranteeing the anonymity of the participating respondents, we sent out initial invitations and a total three reminders. We allowed potential respondents to opt out of future emails every time we reached out to them. Please see Appendix A for the invitation emails.

\centerline{--- Insert Table 1 about here ---}

Table 1 displays our sample selection procedures. Of the 2,660 potential respondents we approached, we excluded 118 for various reasons.[^3] Furthermore, for 206 emails bounced, we received a notification that the email address was unreachable. Of the remaining 2,336 successfully reached accounting researchers, 391 started the study (15,4% of the total emails sent), 254 partially completed the study (10,9% of the total emails sent), and 222 fully completed the study (8,7% of the total emails sent).[^4] Table 2 displays our final sample's demographics and characteristics. Panel A reveals that the majority of the respondents is male, 35 years or older (87.8%), specializes in Financial Accounting and Reporting or Managerial Accounting (70.3%), has Archival or Field Research as their primary research method (55.9%), and is from North America or Europe (91%).

[^3]: 41 authors were removed because we found no email address, 41 authors were removed because they appeared in the data twice with two author IDs but were the same person, 28 authors were removed because they were a current member of one of our affiliated universities or academic research centers, 5 authors were removed because they gave us advice for this study, 2 authors were removed because they had passed away, and 1 author was removed due to an incorrect email address.

[^4]: As the dependent variable of our hypothetical scenario analysis outlined below is the first data that we collect, 254 is the sample size for this analysis. For the descriptive analyses, we settle for the number of participants that answered the respective question and, if applicable, also provided the necessary demographics for cross-sectional splits. Depending on this, the sample sizes vary from 222 to 229 responses. 
\centerline{--- Insert Table 2 about here ---}


Panel B of Table 2 contrasts our respondent sample with our sampling population. Given the anonymity of our response and the data that we are able to collect about our population we are only able to compare the geographic dispersion of our sample. Panel B reveals that, in particular, researchers from Europe are over-represented and researchers from Asia are under-represented in the sample. Given that response rate assessment is inherently hard, we generally abstain from speculating about potential reasons for this sampling imbalance. However, it seems likely that the all-European author team of the study might have had an influence. While we control for geographical effects in our response (not-tabulated) and find only mild variation across regions (researchers from North America tend to state higher sharing activities and preferences), we conclude from this sampling imbalance that our results mostly speak to the attitudes of accounting scholars from North America and Europe and that we can say very little about researchers located in Asia that happen to have a very low response rate.

### Variable Measurement

To ensure that our findings are comparable with related prior work, we adapted key questions from the Survey of Open Science Practices and Attitudes in the Social Sciences [@FLCPSWMBP_2023], which has been recently published in Nature Communications. Very similar to our survey, this study solicits the view by published researchers and PhD students on reproducibilty, research material sharing, and pre-registration. Given that it differs in survey detail (app. 45 minutes while our survey was designed to be completed in less than 10 minutes), and data collection procedure (confidential with compensation element while we use an anonymous survey with very mild incentivization), we had to make some adjustments. However, the ordering and phrasing of questions in our survey are similar and their content and answer scales are identical or differ in a negligible way. Specifically, we decided to alter the framing and expand some of the questions to make it easier for accounting researchers to understand the questions and connect them to their common research practices. We also pre-tested our adapted questions among a group of doctoral students specializing in accounting, which indicated no issues with the adapted questions.

The first stage of the survey included questions about respondents' views on publicly sharing research materials, which we described as comprising of data, code, and research procedures. Specifically, we asked respondents about their familiarity with public sharing of research materials and their views of its importance. We also asked them about their perceptions of the prevalence of sharing in the accounting research community, how often they tended to share, and whether they planned to share in the future. Conditional on whether respondents indicated that they do not always share, we also asked them for the reasons behind not sharing. Lastly, we asked respondents to estimate what percentage of accounting researchers favor sharing and their approximation of the percentage of editors of the top six accounting journals that believe sharing increases the value of a manuscript.

The second stage of the survey featured a selection of questions about respondents' views on reproducibility and pre-registration, which we also borrowed and adapted from @FLCPSWMBP_2023 . First, we asked respondents about their confidence in the reproducibility of accounting research. We also asked them about their subfield within the domain of accounting (i.e., Auditing, Financial Accounting and Reporting, Managerial Accounting, Taxation, Accounting information systems, and Other) and whether they believe the findings in their field are reproducible. Next, we asked the same questions for their methodological area (i.e., Analytical, Archival/Field, Experimental, Qualitative, Survey, and Other). Subsequently, we asked respondents about their familiarity and experience with the practice of pre-registration. Lastly, we asked them about their perceptions of trustworthiness of pre-registered research compared to non-pre-registered research. We completed the survey with asking a few demographic questions that are mostly presented in Table 2. For a complete overview of the survey questions included in our study, please see Appendix A.

## Findings

### Reproducibility

The first goal of the survey was to gauge the perceptions of accounting researchers regarding the reproducibility of influential findings in the accounting literature. Table 3 displays a detailed overview of our respondents' answers to the question "How confident are you that the influential findings in the area of accounting overall are reproducible?" The table also contrasts the answers provided by our respondents with the answers submitted by researchers in other fields in the survey conducted by @FLCPSWMBP_2023 , who asked the question "How confident are you that the influential research findings in \[discipline\] would replicate?" We changed the wording from "replicating" to "reproducing" in our adapted question to make it more recognizable to a diverse group of researchers, such as those in the academic accounting community. Namely, the term "reproducibility" is a focal point in the discussion across the social sciences, including accounting [@B_2016; @CDF_2016; @M_2017; @HLL_2000]. In contrast, the term"replication" may or may not have different interpretations for different researchers. For instance, experimental researchers it refers to distinct activity (i.e., repeating an experimental study), while for archival researchers, it refers to a similar activity (i.e., re-analyzing the same data using the same procedures).


\centerline{--- Insert Table 3 about here ---}

The results in Table 3 suggest that we accounting researchers hold a generally more critical view of the reproducibility of our influential findings. Compared to researchers in the fields of economics and sociology, accounting researchers who took part in our survey are about a half a point more skeptical about the reproducibility of their influential work (on a 5-point Likert scale). This difference is less stark when we compare accounting researchers to researchers in the fields of Political Science and Psychology. In a field fixed effect regression, in which we use Accounting as the baseline, we find that all four fixed field effects are statistically different from zero (two-tailed p-value \< 0.01). Hence, the field of Accounting is generally more skeptical about the reproducibility of its research than other social science fields.


\centerline{--- Insert Figure 1 about here ---}

Figure 1 visually represents the data from Table 3 by plotting them on a horizontal axis and aligning them around a neutral midpoint. The figure shows that accounting researchers are the most skeptical (i.e., "Not at all confident") about the reproducibility of influential findings in their field (10.5%). This difference is much higher than the percentage of skeptics reported by @FLCPSWMBP_2023: 0.7% for Economics, 2.1% for Political Science, and 1.1% for Psychology, and 1.3% for Sociology. The field of Accounting, therefore, also has the lowest percentage of non-skeptics (i.e., researchers who are either moderately or extremely confident in the reproducibility of influential findings). That is, Accounting has about 39.3% non-skeptics, while Economics has 61.1%, Political Science has 45.8%, Psychology has 41.3%, and Sociology has 61.7%. The differences with Economics and Sociology again are the largest, while the differences with Political Science and Psychology are smaller.


Next, we compare assessments of accounting researchers regarding reproducibility across different methodologies. Accounting is not only a diverse discipline in terms of orientation and topic, but also in terms of the methodologies its researchers employ. We distinguish among five categories: Analytical, Archival/Field, Experimental, Qualitative, and Survey. We asked respondents which of those categories best described their main methodological area, enabling us to categorize them in the data. Table 4 displays their assessments regarding the reproducibility of influential findings across three panels. Panel A focuses on assessments of reproducibility within accounting as a whole, Panel B on assessments within their main methodological area, and Panel C on differences in assessments between accounting as a whole and their main methodological area.


\centerline{--- Insert Table 4 about here ---}

The findings in Table 4 Panel A suggest that analytical and qualitative researchers are more pessimistic and skeptical about the reproducibility of influential findings in accounting as whole than accounting researchers using other methodologies. In particular, analytical researchers are significantly less confident about the reproducibility of influential findings in accounting in general than researchers in the Archival/Field and Experimental categories (two-tailed p-value \< 0.100). When turning to respondents' assessments of the reproducibility of influential findings in their own methodological area, we find that analytical researchers have the most confidence in the influential findings within their methodological area as their average confidence is statistically higher from those reported by the average reported for the other four methodologies (two-tailed p-value \< 0.100). Qualitative research reports a significantly lower average confidence than Analytical, Archival/Field, and Experimental research, albeit it is indistinguishable from the average confidence reported by those in the Survey category (two-tailed p-value \> 0.100).

Panel C reports the "assessment gap," which captures the difference in assessments of the reproducibility of influential findings in respondents' own methodological area versus accounting in general. A higher (lower) assessment gap indicates that respondents are more (less) confident in the reproducibility of influential findings in their methodological area compared to accounting in general. The findings of Panel C report that analytical researchers report the largest assessment gap, implying they are relatively skeptical about reproducibility of influential findings in accounting as a whole compared to their own methodology. The average assessment gap reported for the Analytical category is also statistically different from the averages reported in the other four categories (two-tailed p-value \< 0.100). The other four categories exhibit no clear assessment gap (with averages hovering around zero) and are statistically indistinguishable form each other.

### Sharing Research Materials - Familiarity and Importance

Accounting researchers so far show a disproportionate skepticism toward the reproducibility in their field and with some variation across methodologies. Given the skepticism,we have observed so far, one would expect accounting researchers to take a more proactive stance on open science approaches, like the practice of research materials sharing. The survey assessed respondents' evaluations of the practice of research materials sharing next. Specifically, we asked respondents two questions about the practice of sharing research materials, which we defined as the practice of sharing data, code, and research procedures. These two questions were also close adaptions of the survey done by @FLCPSWMBP_2023.

\centerline{--- Insert Table 5 about here ---}

The first question assesses respondents' familiarity with the practice of research materials sharing: "How familiar are you with the practice of publicly sharing research materials (i.e., data, code, and research procedures) for a completed study?"[^5] Table 5 Panel A shows respondents' answers to the question about familiarity, including benchmark answers of respondents in different fields originating from the survey by @FLCPSWMBP_2023 . Generally speaking, academics in the social sciences report being relatively aware of and familiar with the practice of research material sharing. However, the results do suggest that, compared to each of the other four fields in the social sciences, accounting researchers are significantly less familiar with the practice (two-tailed p-value \< 0.010). The second question captures respondents' assessed importance of research material sharing: "To what extent do you believe that publicly sharing research materials (i.e., data, code, and research procedures) is important for the advancement of accounting research?"[^6] Table 5 Panel B suggests that accounting researchers also find the practice of research material sharing less important than academics in each of the other four fields (two-tailed p-value \< 0.010).

[^5]: The corresponding question in @FLCPSWMBP_2023 is "Have you ever heard of the practice of publicly posting data or code online for a completed study?" We deviated from this original question by using the term "research materials" and defining it as "sharing data, code, and research procedures" in our version of the question because we wanted to cover all aspects of this Open Science practice in one question.

[^6]: The corresponding question in @FLCPSWMBP_2023 is "To what extent do you believe that publicly posting data or code online is important for progress in \[Discipline\]?" We adapted this question for terminological consistency across our survey.


\centerline{--- Insert Figure 2 about here ---}

Figure 2 reproduces the data reported in Table 5 Panel B by presenting the frequencies along horizontal axes and centering them to a neutral midpoint. It reveals that, compared to the social science disciplines, relatively less respondents in our sample are strong supporters of the practice of research material sharing. That is, our of the total sample, 39.6% of the respondents consider it "very important" (Accounting) versus 51.7% in Economics, 48.1% in Political Science, and 43.8% in Psychology. Only Sociologists report a lower percentage of strong supporters (i.e., 29.4%) than the accounting researchers in our sample.


\centerline{--- Insert Table 6 about here ---}

Next, we examine familiarity with and perceived importance of research materials sharing across methodologies within accounting. We again distinguish among five methodological areas (i.e., Analytical, Archival/Field, Experimental, Qualitative, and Survey). Table 6 displays assessments of the practice of research material sharing using two panels. Similar to Table 5, Table 6 Panel A reports about the familiarity with the practice (i.e., "How familiar are you with the practice of publicly sharing research materials (i.e., data, code, and research procedures) for a completed study?") and Panel B reports about assessed importance ("To what extent do you believe that publicly sharing research materials (i.e., data, code, and research procedures) is important for the advancement of accounting research?").

Panel A shows that familiarity is larger for respondents using Archival and Field methods compared to respondents using Qualitative, Analytical, and Survey methods (two-tailed p-value \< 0.100). Moreover, we also find that respondents using Qualitative methods are less familiar with research material sharing compared to all four other categories (two-tailed p-value \< 0.100). Panel B shows that there are few significant differences in terms of assessed importance of research material sharing across the methodologies. Only respondents in the Archival/Field category assess research material sharing as significantly more important than respondents using the Qualitative and Survey category.


### Sharing Research Materials - Rationale and Scope

So far, we document that, compared with researchers from other social sciences, we accounting researchers are less familiar with research sharing and find it less important. Since we also wanted to understand what drives accounting researchers' willingness to share research materials, our survey contained a hypothetical scenario in which another (hypothetical) fellow researcher asked respondents whether they would like to share the research materials (i.e., data, code, and research procedures) of a paper they published in an academic journal.

#### Hypothetical Scenario - Design

We manipulated two between-subjects factors in the hypothetical scenario. First, we manipulated the rationale used by the fellow researcher requesting access to the researcher materials. In the *collaboration rationale* treatment, the fellow researcher requested access to researcher materials to understand the work better and build on it. In the *control rationale* treatment, the fellow researcher requested access to verify and potentially correct the research. Second, we manipulated whether the fellow researcher requested the research materials to be shared publicly (i.e., *public scope*) or privately (i.e., *private scope*). Respondents were randomly assigned to receive only one stated rationale (*control* *rationale* versus *collaboration rationale*) and one sharing scope (*public* versus *private scope*). We measured respondents' willingness to comply with the fellow researcher's request on a scale, ranging from 0% (definitely not) to 100% (definitely). The hypothetical scenario was, like the survey, designed using oTree [@CSW_2016].


We predicted that the effect of using a collaborative versus controlling rationale increases accounting researchers' willingness to share their research materials because it communicates a potential benefit rather than cost in terms of reputation (our first hypothesis). We also predicted that a private sharing request leads to a higher willingness to share than a public sharing requests because the former involves a lower potential downside risk in terms of reputation than the latter (our second hypothesis). Finally, we predicted that the effect of stating a collaborative versus controlling rationale is larger in the case of a private sharing request as opposed to a public sharing request (our third hypothesis). The conceptual reasoning behind this interaction effect is that, besides the additive effects of private sharing and a collaboration rationale, the perceived benefit of sharing under the collaboration rationale should be amplified when the scope of sharing is private rather than public. Namely, for a private scope, a collaboration rationale implies more direct benefits of sharing than for a public scope. We pre-registered our predictions and design at the \href{https://osf.io/4akyd}{Open Science Foundation}.


#### Hypothetical Scenario - Procedures

After accepting the terms and conditions, respondents were told that they would read a short scenario and that they would answer questions about it. The scenario included a description that they just conducted a study and published it in an academic journal. Respondents were also told that a fellow researcher contacted them for access of their research materials, which we specified as 'data, code, and research procedures'. Next, we provided four pieces of information about the request made by the fellow researcher. In this part of the scenario, we manipulated whether the request was to make the research materials publicly accessible (*public scope*) or share them only with the requesting fellow researcher (*private scope*). We also featured our second manipulation, specifically, whether the provided rationale was to collaborate with or control the research of the respondent (*collaboration rationale* versus *control rationale*). Appendix B contains an overview of the manipulations and the materials.

\centerline{--- Insert Table 7 about here ---}

#### Hypothetical Scenario - Findings

Table 7 shows the findings of the hypothetical scenario. On average, we find a relatively high willingness to share research materials in the hypothetical scenario and across different variations of rationale and scope (mean = 72%, s.d. = 28%). However, we do not find statistical support for our three predictions. First, Table 7 Column 1 reports no significant effect of using a *collaboration rationale* over a *control rationale* (two-tailed p-value \> 0.100). Moreover, Column 2 shows no support for a main effect of *private scope* on respondents' willingness to share research materials (two-tailed p-value \> 0.100). Lastly, column 3 shows no support for an interaction effect between *collaboration rationale* and *private scope* (two-tailed p-value \> 0.100). In sum, although we find that respondents express a high willingness to share research materials in our hypothetical scenario, we don't find any evidence it is affected by the way the request for sharing is made. At least in our hypothetical scenario, it does not matter whether the request is made to make the materials private (rather than public) and whether the request is made to promote collaboration as a research community or to verify whether the research has been conducted appropriately.

### Sharing Research Materials - Prescriptive versus Descriptive Views and Rationales

Our respondents report a high willingness to share research materials in a hypothetical scenario and do not seem moved by the circumstances under which requests for research materials are made. Next, our survey focuses on the perceived prescriptive and descriptive importance of research material sharing. Table 8 Panel A reports data on respondents' answers to the question "How often do you publicly share research materials (i.e., data, code, and research procedures)?" It also splits the data into main methodological areas (i.e., Analytical, Archival/Field, Experimental, Qualitative, and Survey). The results in Panel A reveal that 50% of the respondents state that they at least sometimes publicly share their research materials. We also find that stated sharing tendencies are significantly lower for respondents in the Qualitative and Survey categories compared to each of the three other methodological areas, i.e., the respondents in the Analytical, Archival, and Experimental categories (two-tailed p-value \< 0.100).


\centerline{--- Insert Table 8 about here ---}

Table 8 Panel B reports answers to three questions measuring respondents' perceptions of how important others consider research material sharing to be (i.e., prescriptive and descriptive importance). Namely, these three questions ask (1) what percentage of accounting researchers in general favor research material sharing, (2) what percentage of editors of the top six accounting journals believe it increases the value of a manuscript, and (3) what percentage of accounting researchers publicly share research materials. The data in Panel B reveal that respondents believe about 44% of accounting researchers favor publicly sharing research materials, they believe 60.8% of the top-six accounting editors to be in favor of publicly sharing research materials, and they believe 27.6% of accounting researchers actually share research materials.



\centerline{--- Insert Figure 3 about here ---}


To understand the differences between these three questions, Figure 3 displays box plots, one for each question and separately for each methodology. First, examining the three box plots for the sample as a whole, the visual pattern emerges that the perceptions of actual sharing behavior among accounting researchers lags behind perceptions of whether accounting researchers favor sharing, which, in turn, lags behind perceptions of whether editors value sharing. This pattern seems starker for respondents in the Archival/Field category. In this category, the box plot for the perceptions of actual sharing does not to overlap with the other two, i.e., the box plots for the perceptions of accounting researchers favoring sharing and the perceptions of whether editors value sharing. These findings suggest a contrast in the perceived (prescriptive) importance of research material sharing and the perceived (descriptive) actual sharing behavior.

### Sharing Research Materials - Rationales

So, what prevents accounting researchers from sharing? Our survey also included questions about the rationales against sharing. When respondents reported that they did not always publicly share research materials, we asked them which reasons would make them withhold their research materials. Respondents were able to provide more than one reason.


\centerline{--- Insert Table 9 about here ---}

Table 9 Panel A displays the frequencies and percentages for each reason featured and provided in our survey. Aside from reporting the total frequencies for the entire sample, it also (again) splits them up into the five methodological fields (i.e., Analytical, Archival/Field, Experimental, Qualitative, and Survey). The most commonly stated reason that respondents gave for not sharing research materials is that the data are proprietary. 56.3% of all respondents give this reason and it also ranks first for all methodological areas. This is somewhat surprising because not all methodological areas regularly deal with proprietary data. In a shared second place, are three more prominent reasons respondents provided, namely, (1) the time that it takes to make materials available (29.9% of all respondents), (2) the fear of being scooped when materials become publicly available (23.2% of all respondents), and (3) no need for sharing as their data are already publicly available (33.9% of all respondents). There are some other notable differences across the methods. For qualitative researchers, deidentification problems are a relatively important reason (45.8%) compared to the others. Also, analytical researchers consider the time that it takes to make materials available far less of a reason than researchers using the other methods (i.e., 9.5%).

Besides this multiple choice questions soliciting the reasons for not sharing research materials, we also ask our participants to reflect on their answer in our hypothetical sharing scenario. The answers that they provided in this free form field have been coded by a research assistant that was not informed about the experimental design of the study. The resulting categories characterizing the coded answers and their respective frequencies are presented in Panel B, for the total sample as well as for the different treatment cells.

Different from the findings in Panel A, we are now also able to identify reasons for and not just against sharing. First and foremost our participants share because they believe that this enhances the quality of research. Another strong argument is that it helps other researchers. This argument is significantly more likely to be provided by participants that faced the collaboration treatment, in line with our theoretical predictions and providing evidence that the framing of the sharing situation has an effect on the decision-making of our participants.

The main rationales against sharing are related to the materials being considered valuable intellectual property that can potentially be reused for other research projects. A typical example for such a statement is:

\begin{displayquote}
"There might have been substantial workload flown into collecting data that also will benefit follow-up projects. Giving away the data might harm future projects without remuneration for the collection effort."
\end{displayquote}

Another interesting aspect, that does not become apparent from the pre-conditioned analysis in Panel A, is that a sizable amount of participants indicate that they are concerned about unethical use of their research materials. These concerns are related to other researchers using their work for "reverse p-hacking", trying to discredit prior findings to establish a contribution. Again, an example for such a statement:

\begin{displayquote}
"It sounds like they are trying to disprove me. They're on a witch hunt. There's a good chance they are doing this to a lot of people and are, themselves, going to suffer from a file drawer problem. If they re-run 100 people's studies and find that 1 doesn't replicate, that's the one they will publish and discredit and shame the researcher, ignoring the fact that it was likely due to chance, and not noting the 99 who got it right."
\end{displayquote}

But even without reverse p-hacking, research material sharing requests can create career concerns. An example:

\begin{displayquote}
"I would want to share my materials but I would be nervous that I did something 'wrong'  and it would make me look bad if my study didn't replicate. I'm a junior faculty and wouldn't want to do anything to jeopardize my reputation or likelihood of getting tenure."
\end{displayquote}

Another aspect, which also featured prominently in Panel A, is the time that it takes to entertain research material sharing and potential followup requests. For example:

\begin{displayquote}
"Many academics, including me, are constantly working under high pressure to deliver performance and short of time to fulfill a large range of commitments. It is simply impossible to entertain each and every such request, especially when replying to the first might lead to feeling the obligation to answer more questions from the requester in the future. A requester can be a relatively inexperienced graduate student exploring a doctoral topic, which has implications to how many questions s/he might have in the future. Thus, the propensity to reply to a request often depends on the status of the requester in the field and the anticipated consequences (or the lack of) of entertaining the request (or not)."
\end{displayquote}

The last aspect of the this quote also hints at a rationale that was commonly featured in the "it depends" category: the personality of the requesting person and their reputation. A final example:

\begin{displayquote}
"It depends on whether I know the person and their reputation in the research community. I think my reputation would be tarnished if I didn't provide but I would also be skeptical of providing someone I didn't know my hand-collected data and code that took years of work to produce."
\end{displayquote}

These statements indicate the potential for selective sharing among well-connected researchers, which would benefit an "in-crowd" of academics while at the same time unleveling the academic playing field and increasing  inequality. Taken together, we conclude from the rationale analysis that our participants give more consideration to potential sharing costs then they give to potential sharing benefits. While the sharing benefits become more prevalent with a collaboration framing, the sharing costs are prominent regardless of the hypothetical scenario.


### Pre-registration

Our survey has so far revealed that accounting researchers who participated in our survey are more skeptical about reproducibility than other social science areas. We also find that they are less familiar with research materials sharing as an open science practice that helps address reproducibility and that they find that practice less important. There seem to be various reasons why accounting researchers share and even more why they do not share their research materials. Besides research material sharing, our survey also covered a second Open Science practice: Pre-registration.


\centerline{--- Insert Table 10 about here ---}

Table 10 lists the frequencies and percentages for three questions about pre-registration. Panel A considers respondents' familiarity with the practice itself. Namely, we asked respondents: "How familiar are you with the practice of pre-registering hypotheses or analyses in advance of a study?"[^7] The results in Panel A suggest accounting researchers are equally familiar with the practice as researchers in Political Science and Psychology. They seem to be slightly more familiar than researchers in Economics and Sociology (two-tailed p-value \< 0.010). While this finding might appear surprising at first glance one has to factor in that the practice of pre-registration is gaining traction over time and our survey was fielded two years later than the second wave of our benchmark survey.

\centerline{--- Insert Figure 4 about here ---}

Panel B reports data on the first year respondents pre-registered. Namely, we asked them: "Approximately when was the first time you pre-registered hypotheses or analyses in advance of a study?"[^8] The findings exhibit a clear pattern: Accounting researchers started considering pre-registration for the first time significantly later than researchers in other social science fields (two-tailed p-value \< 0.010). The take-up started around 2018 compared to 2015-2016 for the other fields. This finding is also supported by Figure 4 showing that the take-up of pre-registration in accounting over time lags behind the other social science areas.

Panel C shows data on answers to the question: "How trustworthy do you find the work of scholars who tend to pre-register hypotheses or analyses as compared to the work of those who don't?"[^9] Results indicate no clear differences in perceived trustworthiness in accounting compared to economics and political science (two-tailed p-value \> 0.100). However, the perceived increase in trustworthiness when pre-registering is less in accounting compared to psychology and sociology (two-tailed p-value \< 0.010).

[^7]: The question used in the survey of @FLCPSWMBP_2023 is "Have you ever heard of the practice of pre-registering hypotheses or analyses in advance of a study?".

[^8]: The question used in @FLCPSWMBP_2023 was "Approximately when was the first time you pre-registered hypotheses or analyses in advance of a study?"

[^9]: The question used in the survey of @FLCPSWMBP_2023 is "What is your opinion of pre-registering hypotheses or analyses?". We found this question to be quite broad yet it used a trustworthiness scale. So, we chose more specific framing for our question.


\centerline{--- Insert Table 11 about here ---}

In this final part of our analysis, we examine differences in familiarity, usage, and trustworthiness of pre-registration across the different methodological areas within accounting. Table 11 displays respondents' frequencies using three panels. Similar to Table 10 Panel A, Panel A of Table 11 reports on familiarity, i.e., the question: "How familiar are you with the practice of pre-registering hypotheses or analyses in advance of a study?" Somewhat unsurprisingly, Panel A shows that qualitative accounting researchers are less familiar with the practice than accounting researchers belonging to the Archival, Experimental, and Survey categories (two-tailed p-value \< 0.100). Yet, there is no statistical difference between qualitative and analytical accounting researchers. Panel B reports on differences in usage, specifically, the question "`r vars$text[100]`", similar to the question featured in Table 10 Panel B. We find that accounting researchers in the experimental field report the highers usage of pre-registration, while none of the qualitative accounting researchers report using it. Lastly, Panel C reports differences in perceived trustworthiness. Specifically, it features answers to the question "How trustworthy do you find the work of scholars who tend to pre-register hypotheses or analyses as compared to the work of those who don't?" which is identical to the question featured in Table 10 Panel C. We find no clear evidence of differences among the different methodologies in whether they trust pre-registered work more than work that has not been pre-registered.

## Discussion and Conclusion

Our results support the conclusion that us accountants face an "open science dilemma". On the one hand, we are more skeptical about the reproducibility of our influential findings than other social science researchers are about their respective findings. On the other hand, we seem to be a little bit behind the curve in terms of research material sharing and research design pre-registration.

If these conclusions are descriptive: What should be our reaction? While we are not in the position to advise our profession on how to proceed, we feel that a two-pronged approach might be in order. First: Why are we so skeptical about our own work? Is this an example of the anecdotal "professional skepticism" that leads us accountants to over-weight bad signals in general? Or do we indeed have good reasons to be concerned? The only way to find out is to take stock.

As @LSA_2024 indicate, reproduction studies in accounting are not rare, and a majority of those confirms prior results. However, we need more systematic evidence on whether our findings reproduce and also hold in new settings and/or with new data. A very impressive exercise in that regard is the recent large-scale reproduction attempt of studies published in Management Science [@FGHKO_2023] that systematically reproduced almost 500 articles that were published in Management Science before and after their establishment of a Data and Code Policy in 2019. One interesting finding from that study is that accounting studies in Management Science are -if anything- more reproducible than studies in neighboring fields like finance. Based on this it seems as if professional skepticism might indeed be partly responsible for our open science dilemma.

It would be insightful to compare the findings for Management Science, which also document an impressive effect of their data and code policy, with data on studies published in the Journal of Accounting Research, again pre and post their data policy. These findings could in turn then be compared with those for studies published in other leading journals that do not have a dedicated data and code policy (yet). But not only systematic large-scale reproduction studies can update our priors about the reproducibility of our work. Replication games, potentially embodied in doctoral education, are an alternative means to assess the reproducibility of our work while at same time helping researchers to grow their network and empirical skill-sets alike. Taken together, we believe that the academic accounting profession should be well positioned to take a leading role in verifying its own output. In the end, assessing the reproducibility of scientific work is an assurance and auditing process, an area we know quite a bit about!

The second element of our two-pronged approach would be to promote research material sharing and pre-registration, potentially in combination with other open science measures that are aiming to increase the accessibility and re-usability of scientific work. As economists, we are quick to jump on the incentive problems in this regard. Why should we do it? Does sharing code and data help to build academic careers? While some associative evidence from other fields indicates that papers that share their data are cited more often [@PDF_2007], currently, as hard as it seems, the answer to this is not clearly affirmative. Research material sharing requires skills and time, and, to the extreme, devoting this time might even be regarded as a negative signal by tenure and promotion committees, assuming that it crowds out time spent on producing "more tenurable output".

To us, this mindset seems to be the core of the the problem. We believe that it is misguided as research material sharing constitutes a public good that is crucial for scientific progress. Think about a scientific computing world without open source contributions. Without the BSD/Linux operating system and its tools, without Python, R, git, Latex, and the accompanying library ecosystem. The scientific advancements in areas like Econometrics, Machine Learning, and Natural Language Processing would be -if at all feasible- severely delayed. As our field builds on the insights of these areas, we benefit from the open science paradigm in our every day work.

Given that, one might wonder whether data and code sharing by accounting researchers can create similar positive externalities. As our comparative competitive advantage lies less in methodological aspects but more in institutional expertise, we sense potential candidates in the area of carefully curated and pre-processed data building on the universe of firm disclosures. Prominent examples include curated word lists for sentiment analyses in financial settings [e.g., @LM_2011], topic model-based measures of specific firm risks [e.g., @H_2019], and industry classifications based on financial disclosures [e.g., @HP_2016].

A closely related area where we see the potential of accounting data to develop an impact is the collection and pre-processing of web and electronic filing data. For the former, web scraping of corporate webpages can provide informative data [e.g., @BBB_2021] and for the latter, in particular the upcoming electronic filing requirements for sustainability reports hold promise. While in this area, researchers are competing with established commercial data providers, the need of research to study also unstructured textual data makes it necessary and useful to build research-focused data pipelines and products instead of simply trusting (and paying) commercial data vendors. An additional aspect of this is that freely licensable research data can remove one of the largest entry hurdles for empirical archival accounting researchers.

While these flagship projects are useful showcases that data and code sharing in the accounting domain can be very impactful also for other domains, in order to promote reproducible workflows overall we need to make the case that adopting such a workflow not only contributes to the public good by making research materials accessible but also enhances research efficiency at the individual level. In order to be able to share code and data, one has to adopt a workflow that integrates reusability aspects into project development. The data and code requirements of leading journals in economics, finance, and accounting are not only pushing the availability of research materials but also impose substantial costs on researchers that have to provide more or less complete replication package upon acceptance. As many affected by these regulations will be open to admit, transforming an empirical project from an internal "well it works" scripting exercise to a well documented replication package that ideally works across multiple platforms is far from trivial. This process becomes significantly easier and also more enjoyable if an open science oriented workflow is adopted throughout all stages of project development. Besides this, it also makes it easier for co-author teams to cooperate.

As lack of time has been identified as one of the biggest roadblocks against research material sharing in our survey, educational activities targeted at the PhD-level in scientific computing and project management in general seem important. While well-funded research institutions might have the opportunity to outsource these skills to research professionals, the vast majority of researchers world-wide need to acquire them themselves. This is not necessarily a bad thing, as we believe that having these skills as a researcher enables one to develop more creative research projects compared to researchers that are unaware of what is feasible with state-of-the-art scientific computing. In the old days, many accounting PhD programs were implicit- or explicitly relying on scientific computing skills to be handed down from one PhD generation to another as tacit knowledge. It seem that these days are over and that dedicated scientific computing courses should become as a central component of the scientific education in the quantitative social sciences as lab training is in the natural sciences.

A lot of the above applies mostly to the empirical and quantitative areas of our discipline. What about analytical and qualitative researchers and their studies? As becomes apparent from our findings, their views on open science differ quite prominently from the "accounting mainstream". Analytical researchers seem to be much more confident in their own work, potentially in line with theoretical work being easier to verify during the review process. Then again, in order to develop the impact that it deserves, analytical work not only needs to be reproducible but also accessible and reusable, ideally also by non-expert researchers from neighboring methods. Qualitative work, on the other hand, faces challenges that sets it apart from typical quantitative work. First, its data is often collected under confidentiality, legally restricting future sharing. Second, its unique richness makes it inherently hard to share across subjects, especially if re-identification issues are present. Third, related and maybe more epistemological in nature, the assumption of objectivity, which allows research materials to be detached from the individual researcher and to be re-used by others, is questioned within the qualitative realm. This implies that qualitative scholars have to develop -and are developing [e.g., @M_2014; @FRV_2022]- their own understanding what reproducibility means within their methodological paradigm.

Finally, fears and a general lack of trust in fellow researchers also seem to be important deterrents to the adoption of open science methods. This is human but unfortunate, also because it leads to subtle "home biases" when it comes to sharing, potentially increasing academic inequality and hindering scientific progress. While the fear of being "scooped" can be addressed by publishing materials later in the publication progress or publicly claiming intellectual ownership to ideas via registration and use of well-established repositories, fears with regards to future researchers pointing to deficiencies in ones work and/or a general lack of trust are harder to overcome. For both aspects, we need to work towards changing the error climate in the academic discipline. Also, we believe that it might be helpful to move away from the "policing" aspect of reproducibility and instead stress the collaborative public good character of open science activities. With them, we can help each other, promote our field and, yes, make the research process even more fun!

\newpage

## References {.unnumbered}

\singlespacing

\small

::: {#refs}
:::

\doublespacing

\newpage

## Figures {.unnumbered}

### Figure 1: Reproducibility of Influential Findings Across the Social Sciences {.unnumbered}

```{r Figure1, fig.width=8, fig.height=5}
likert_plot(mr, "reprod", "demog_discipline", title_str = "How confident are you that the influential reserach findings in your area will replicate/reproduce?", llevels = vars$resp_label[80:84])
```

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```
This figure is based on our survey data merged with the data collected by @FLCPSWMBP_2023. The question asked in our survey is "`r vars$text[80]`" while the original question used in the survey of @FLCPSWMBP_2023 is "How confident are you that the influential research findings in \[Discipline\] would replicate?". The data of @FLCPSWMBP_2023 has been collected in two waves (2018 and 2020) from a population of researchers located in the U.S. Our survey was conducted in 2022 among accounting researchers world-wide. Our survey and the merging process of the data are discussed in more detail in section 2 of the paper.

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
```
\newpage

### Figure 2: Importance of Research Material Sharing Across the Social Sciences {.unnumbered}

```{r Figure2, fig.width=9, fig.height=5}
mr$sharing_important_rescaled = floor(mr$sharing_important)
likert_plot(mr, "sharing_important_rescaled", "demog_discipline", title_str = "To what extent do you believe that publicly sharing research materials is important?", llevels = vars$resp_label[49:53])
```

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```
This figure is based on our survey data merged with the data collected by @FLCPSWMBP_2023. The question asked in our survey is "`r vars$text[49]`" while the original question used in the survey of @FLCPSWMBP_2023 is "To what extent do you believe that publicly posting data or code online is important for progress in \[Discipline\]?". The data of @FLCPSWMBP_2023 has been collected in two waves (2018 and 2020) from a population of researchers located in the U.S. Our survey was conducted in 2022 among accounting researchers world-wide. Our survey and the merging process of the data are discussed in more detail in section 2 of the paper.

```{=tex}
\end{singlespace}
\setlength{\parindent}{4em}
```
\newpage

### Figure 3: Sharing of Research Materials: Prescriptive versus Descriptive Views {.unnumbered}

```{r Figure3, fig.width=8, fig.height=6}
pres_desc_plot(sd)
```

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```
This figure is based on our survey data and summarizes the findings from three separate questions. The first question, related to the preferences of editors is "`r vars$text[79]`". The second question, related to the preferences of other researchers, is "`r vars$text[78]`". The third question, related to the the actual behavior of other researchers, is "`r vars$text[54]`". Our survey is discussed in more detail in section 2 of the paper.

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
```
\newpage

### Figure 4: Pre-Registration: Take-up over Time Across the Social Sciences {.unnumbered}

```{r Figure4, fig.width=8, fig.height=6}
pre_reg_over_time_plot(mr)
```

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```
This figure is based on our survey data merged with the data collected by @FLCPSWMBP_2023. The question of our survey is "`r vars$text[102]`". The question used in the survey of @FLCPSWMBP_2023 is "Approximately when was the first time you pre-registered hypotheses or analyses in advance of a study?". The data of @FLCPSWMBP_2023 has been collected in two waves (2018 and 2020) from a population of researchers located in the U.S. Our survey was conducted in 2022 among accounting researchers world-wide. Our survey and the merging process of the data are discussed in more detail in section 2 of the paper.

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
\newpage
```
```{=tex}
\renewcommand{\thetable}{\arabic{table}}
\setcounter{table}{0}
```
## Tables {.unnumbered}

### Table 1: Sample Composition {.unnumbered}

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```
|                                 |                    N (%) |
|:--------------------------------|-------------------------:|
| Authors identified (Raw Sample) |                    2,660 |
| \- Excluded or missing          | 118 (4.4% of identified) |
| = Emails sent                   |                    2,542 |
| \- Emails bounced               |       206 (8.1% of sent) |
| = Emails received               |                    2,336 |
| Surveys Started                 |      391 (15.4% of sent) |
| Surveys Partially Completed     |      254 (10.9% of sent) |
| Surveys Fully Completed         |       222 (8.7% of sent) |

This table reports on our sample composition. Starting with a list of authors that we identified by Scopus to have published in the "Top 6" accounting journals (Accounting, Organizations and Society, Contemporary Accounting Research, Journal of Accounting and Economics, Journal of Accounting Research, Review of Accounting Studies, The Accounting Review) over the period 2016 to 2021, we excluded all authors with missing email information. We recorded any email bounce messages that we received but report the response rate based on the emails that we sent out. Our survey is discussed in more detail in section 2 of the paper.

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
```
\newpage

### Table 2: Sample Demographics {.unnumbered}

#### Panel A: Sample Descriptives {.unnumbered}

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```
```{r Tab2aDemographics, asis = TRUE}
extract_df <- function(dvar, title) {
	df <- create_demog_table(sd, dvar, df_only = TRUE)
	names(df) <- c("Characteristic", "n", "%")
	bind_rows(
		tibble(
			Characteristic = title,
			n = "",
			`%` = ""
		),
		df %>% mutate(Characteristic = paste0("- ", Characteristic))
	)
}

tab <- bind_rows(
	extract_df("demog_field", "Subfield"),
	extract_df("demog_method", "Method"),
	extract_df("demog_age", "Age Group"),
	extract_df("demog_gender", "Gender"),
	extract_df("demog_region", "Geographic Region")
)
gt(tab) %>% as_latex()

```

\newpage

#### Panel B Regional Representativeness {.unnumbered}

```{r Tab2bGeoRep, asis = TRUE}
tab <- create_regions_rep_table(pop, sd)
tab_latex <- gt(tab) %>% 
	cols_align("right", -1) %>% 
	tab_spanner(label = "Population", columns = starts_with("Population")) %>%
	tab_spanner(label = "Sample", columns = starts_with("Sample")) %>%
	cols_label(
    ends_with(" n") ~ "n",
    ends_with(" %") ~ "%",
    ends_with("Rate") ~ "Regional RR"
	) %>%
		as_latex() 
# tab_latex[1] <- sub("\\\\toprule\\n", "", tab_latex)
tab_latex[1] <- sub("\\{Sample\\} &  ", "{Sample} &  \\\\multirowcell{2}{Regional \\\\\\\\ Response Rate}", tab_latex)
sub("& Regional RR", "& ", tab_latex)
```

This table details the demographics of our sample. As respondents were free not to answer any given question, the total number of answers vary by demographic. In Panel B, we contrast the geographical distribution of our sample with the geographical distribution of the survey population. For this, we use the affiliation of our authors as indicated by Scopus. For `r sum(is.na(pop$affil_country))` cases, the affiliation's country is unavailable on Scopus. Our survey is discussed in more detail in section 2 of the paper.

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
\newpage
```
### Table 3: Reproducibility of Influential Findings Across the Social Sciences {.unnumbered}

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```
How confident are you that the influential research findings in your field would reproduce/replicate?

```{r Tab3ReprodSocSciResp, asis = TRUE}
tab <- create_pct_cross_tab_table(mr, "reprod", "demog_discipline", get_resp_label("reprod_acc"), df_only = TRUE)
gt(tab) %>% as_latex()
```

This table is based on our survey data merged with the data collected by @FLCPSWMBP_2023. The question asked in our survey is "`r vars$text[80]`" while the original question used in the survey of @FLCPSWMBP_2023 is "How confident are you that the influential research findings in \[Discipline\] would replicate?". The data of @FLCPSWMBP_2023 has been collected in two waves (2018 and 2020) from a population of researchers located in the U.S. Our survey was conducted in 2022 among accounting researchers world-wide. Our survey and the merging process of the data are discussed in more detail in section 2 of the paper. The asterisks report the results of an OLS regression regressing the response on a series of field fixed effects with Accounting acting as the base field. (\*/\*\*/\*\*\*) indicate two-sided significance at the (10%/5%/1%) significant level, respectively.

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
\newpage
```
### Table 4: Reproducibility of Influential Findings: Assessment Across Methods {.unnumbered}

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```
#### Panel A: General Assessment by Method {.unnumbered}

```{r Tab4aReprMethod, asis = TRUE}
tab <- create_pct_cross_tab_table(
	sd, "reprod_acc", "demog_method", 
	ctests = TRUE, df_only = TRUE
) 
gt(tab %>% select(-Other)) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

#### Panel B: Method-Specific Assessment {.unnumbered}

```{r Tab4bReprMethod, asis = TRUE}
tab <- create_pct_cross_tab_table(
	sd, "reprod_method", "demog_method", 
	ctests = TRUE, df_only = TRUE
) 
gt(tab %>% select(-Other)) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

#### Panel C: Assessment Gap (Method - General) {.unnumbered}

```{r Tab4cReprMethod, asis = TRUE}
sd$reprod_method_delta <- as.numeric(sd$reprod_method) - as.numeric(sd$reprod_acc)
tab <- create_pct_cross_tab_table(
	sd, "reprod_method_delta", "demog_method", 
	resp_label = sort(na.omit((unique(sd$reprod_method_delta)))),
	ctests = TRUE, add_num_labels = FALSE, df_only = TRUE 
) 
gt(tab %>% select(-Other)) %>%
	cols_align("right", -1) %>% 
	as_latex()
```

This table is based on our survey data and two separate questions. The response presented in Panel A is based on the question "`r vars$text[80]`" while the response presented in Panel B is based on the question "`r vars$text[90]`". Panel C reports the individual level differences, measured in likert steps, between the answers given for Panel B and Panel A. In all Panels, answers from two respondents who classified their method as "Other" are excluded. Methods where the answers differ at the two-sided 10%-level are indicated by their first two letters.

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
\newpage
```
```{=tex}
\begin{landscape}
\begin{singlespace}
\setlength{\parindent}{0em}
```
### Table 5: Sharing of Research Materials Across the Social Sciences {.unnumbered}

#### Panel A: Familiarity {.unnumbered}

```{r Tab5aSharingHeardSocSciResp, asis = TRUE}
tab <- create_pct_cross_tab_table(
	mr, "sharing_heard", "demog_discipline", 
	resp_label = c("No (0)", "Yes (1)"), 
	add_num_labels = FALSE, df_only = TRUE
) 
gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

#### Panel B: Assessed Importance {.unnumbered}

```{r Tab5bSharingImportanceSocSciResp, asis = TRUE}
tab <- create_pct_cross_tab_table(
	mr, "sharing_important_rescaled", "demog_discipline", 
	resp_label = get_resp_label("sharing_important"), df_only = TRUE
) 
gt(tab ) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

This table is based on our survey data merged with the data collected by @FLCPSWMBP_2023. For Panel A, the question of our survey is "`r vars$text[44]`" where we code the response "Not at all familiar" as "No". The question used in the survey of @FLCPSWMBP_2023 is "Have you ever heard of the practice of publicly posting data or code online for a completed study?". For Panel B, the question of our survey is "`r vars$text[49]`". The question used in the survey of @FLCPSWMBP_2023 is "To what extent do you believe that publicly posting data or code online is important for progress in \[Discipline\]?" The data of @FLCPSWMBP_2023 has been collected in two waves (2018 and 2020) from a population of researchers located in the U.S. Our survey was conducted in 2022 among accounting researchers world-wide. Our survey and the merging process of the data are discussed in more detail in section 2 of the paper. The asterisks report the results of an OLS regression regressing the response on a series of field fixed effects with Accounting acting as the base field. (\*/\*\*/\*\*\*) indicate two-sided significance at the (10%/5%/1%) significant level, respectively.

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
\newpage
```
### Table 6: Sharing of Research Materials Across Methods {.unnumbered}

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```
#### Panel A: Familiarity {.unnumbered}

```{r Tab6aSharingFamiliarity, asis = TRUE}
df <- sd %>% filter(demog_method != "Other")
tab <- create_pct_cross_tab_table(
	df, "sharing_familiar", "demog_method", 
	ctests = TRUE, df_only = TRUE
) 
gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

#### Panel B: Assessed Importance {.unnumbered}

```{r Tab6bSharingImportance, asis = TRUE}

tab <- create_pct_cross_tab_table(
	df, "sharing_important", "demog_method", 
	ctests = TRUE, df_only = TRUE
) 
gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

This table is based on our survey data and two separate questions. The response presented in Panel A is based on the question "`r vars$text[44]`" while the response presented in Panel B is based on the question "`r vars$text[49]`". In all Panels, answers from two respondents who classified their method as "Other" are excluded. Methods where the answers differ at the two-sided 10%-level are indicated by their first two letters.

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
\end{landscape}
\newpage
```
### Table 7: Scenario Analysis: Hypothetical Sharing Likelihood {.unnumbered}

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```
#### Panel A: Descriptive Statistics {.unnumbered}

```{r Tab7aExpDescriptives, asis = TRUE}
stats <- sd %>%
	group_by(exp_tment_control, exp_tment_private) %>%
	select(exp_tment_control, exp_tment_private,exp_pct_share) %>%
	summarise(
		N = sprintf("%d", n()),
		Mean = sprintf("%.1f%%", mean(exp_pct_share)),
		`Std. Dev.` = sprintf("%.1f%%", sd(exp_pct_share)),
		Minimum = sprintf("%.1f%%", min(exp_pct_share)),
		`25%` = sprintf("%.1f%%", quantile(exp_pct_share, 0.25)),
		Median = sprintf("%.1f%%", median(exp_pct_share)),
		`75%` = sprintf("%.1f%%", quantile(exp_pct_share, 0.75)),
		Maximum = sprintf("%.1f%%", max(exp_pct_share))
	) %>%
	ungroup() %>%
	mutate(
		Rationale = c(rep("Collaboration", 2), rep("Control", 2)),
		Scope = c(rep(c("Public", "Private"), 2))
	) %>%
	select(-exp_tment_control, -exp_tment_private) %>%
	select(Rationale, Scope, everything())
tab <- create_pct_cross_tab_table(
	sd, "reprod_acc", "demog_method", 
	ctests = TRUE, df_only = TRUE
) 
gt(stats) %>% 
	cols_align("right", -1:-2) %>% 
	as_latex()
```

#### Panel B: Regression Results {.unnumbered}

```{r Tab7bExpRegResults, asis = TRUE}
sd$exp_tment_collab <- 1 - sd$exp_tment_control
modh1 <- lm(exp_pct_share ~ exp_tment_collab, data = sd)
modh2 <- lm(exp_pct_share ~ exp_tment_private, data = sd)
modh3 <- lm(exp_pct_share ~ exp_tment_collab*exp_tment_private, data = sd)
tab_latex  <- modelsummary(
	list(H1 = modh1, H2 = modh2, H3 = modh3), output = "gt",
	coef_rename = function(x) {
		x <- str_replace(x, fixed("exp_tment_collab"), "Collaboration rationale")
		x <- str_replace(x, fixed("exp_tment_private"), "Private scope")
		str_replace(x, fixed(":"), "×")
	},
	gof_map = list(
		list(raw = "adj.r.squared", clean = "Adjusted R2", fmt = 3),
		list(
			raw = "nobs", clean = "Number of observations", 
			fmt = function(x) format(x, big.mark=",")
		)
	),
	stars = c('*' = .1, '**' = 0.05, '***' = .01),
	estimate  = "{estimate}{stars}",
) %>% as_latex()

# The below is needed since some tex environment chock on the longtable environment
# See https://github.com/vincentarelbundock/modelsummary/issues/50
tab_latex[1] <- sub("\\(Intercept\\)", "Intercept", tab_latex) 

sub("Adjusted R2", "\\\\midrule\\\\addlinespace[2.5pt]\nAdjusted R²", tab_latex)
```

This table reports the results from the pre-registered experimental design contained in our survey. Is is based on a hypothetical code and data sharing scenario were we manipulated the sharing rationale (collaboration versus control) and the sharing scope (private versus public). All experimental materials are detailed in the Online Appendix of this study. The outcome variable is the stated likelihood that the respondent would share their code and data under the hypothetical scenario. Panel A reports the descriptive statistics for this outcome, conditional for the different treatment cells. Panel B reports the results from OLS regressions of the outcome on the binary treatments (H1 and H2) and their interaction (H3).

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
\newpage
```
### Table 8: Sharing of Research Materials: Prescriptive versus Descriptive Views {.unnumbered}

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```
#### Panel A: Stated sharing frequency across fields {.unnumbered}

```{r Tab8aSharingUse, asis = TRUE}

tab <- create_pct_cross_tab_table(
	df, "sharing_use", "demog_method", 
	ctests = TRUE, df_only = TRUE
) 
gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

#### Panel B: Sharing preference and sharing behavior believes across fields {.unnumbered}

```{r Tab8bSharingPreDes, asis = TRUE}
stats <- df %>%
	select(
		demog_method, sharing_perc_other_prefer, 
		sharing_perc_editor_prefer, sharing_perc_other_share
	) %>%
	na.omit() %>%
	group_by(demog_method) %>%
	pivot_longer(-1, names_to = "var", values_to = "values") %>%
	group_by(var, demog_method) %>%
	summarise(
		Mean = sprintf("%.1f%%", mean(values)),
		`Std. Dev.` = sprintf("%.1f%%", sd(values)),
		`25%` = sprintf("%.1f%%", quantile(values, 0.25)),
		Median = sprintf("%.1f%%", median(values)),
		`75%` = sprintf("%.1f%%", quantile(values, 0.75)),
	) %>% 
	pivot_longer(-1:-2, names_to = "stat", values_to = "values") %>%
	pivot_wider(
		id_cols = c(var, stat), 
		names_from = demog_method, values_from = "values"
	) %>% 
	ungroup() %>%
	select(-var) %>%
	rename(" " = stat)

tab <- rbind(
	c("General Preference for Sharing", rep("", 5)),
	stats[6:10,],
	c("Editor Preference for Sharing", rep("", 5)),
	stats[1:5,],
	c("Believes About Actual Sharing Behavior", rep("", 5)),
	stats[11:15,]
)

tab_latex <- gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()

tab_latex <- sub("General Preference for Sharing &  &  &  &  &", "\\\\multicolumn{6}{l}{General Preference for Sharing}", tab_latex)
tab_latex <- sub("Editor Preference for Sharing &  &  &  &  &", "\\\\multicolumn{6}{l}{Editor Preference for Sharing}", tab_latex)
sub("Believes About Actual Sharing Behavior &  &  &  &  & ", "\\\\multicolumn{6}{l}{Believes About Actual Sharing Behavior}", tab_latex)

```

This table is based on our survey data and several separate questions. The response presented in Panel A is based on the question "`r vars$text[55]`" while the response presented in Panel B is based on the questions "`r vars$text[78]`" (general preference for sharing), "`r vars$text[79]`" (editor preference for sharing) and "`r vars$text[54]`" (actual sharing behavior). In all Panels, answers from two respondents who classified their method as "Other" are excluded. Methods where the answers differ at the two-sided 10%-level are indicated by their first two letters.

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
\newpage
```
### Table 9: Sharing of Research Materials: Rationales {.unnumbered}

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```
#### Panel A: Reasons against Sharing (Close-Ended Question) {.unnumbered}

```{r Tab9aSharingReasons, asis = TRUE}
tab <- sd %>%
	select(starts_with("sharing_reason"), -starts_with("sharing_reason_other")) %>% 
	na.omit() %>%
	pivot_longer(everything(), names_to = "reason", values_to = "val") %>%
	group_by(reason) %>%
	summarise(
		sum = sum(val),
		Total = sprintf("%d (%.1f %%)", sum(val), 100*sum(val)/n())
	) %>%
	arrange(-sum) %>% 
	select(-sum) %>%
	left_join(
		sd %>%
			select(demog_method, starts_with("sharing_reason"), -starts_with("sharing_reason_other")) %>% 
			filter(demog_method != "Other") %>%
			na.omit() %>%
			pivot_longer(-1, names_to = "reason", values_to = "val") %>%
			group_by(demog_method, reason) %>%
			filter(sum(val) != 0) %>%
			summarise(
				stat = sprintf("%d (%.1f %%)", sum(val), 100*sum(val)/n())
			) %>%
			pivot_wider(id_cols = reason, names_from = demog_method, values_from = stat), 
		by="reason"
	) %>%
	mutate(
		reason =str_remove(reason, fixed("sharing_reason_")) %>% 
				str_replace_all(fixed("_"), " ") %>% str_to_sentence()
		) %>%
	rename(Reason = reason)

gt(tab) %>% 
	cols_align("right", -1) %>% 
	sub_missing(missing_text = "") %>%
	as_latex()
```

```{=tex}
\newpage
\begin{landscape}
```
#### Panel B: Rationales about Sharing (Open-Ended based on Hypothetical Scenario) {.unnumbered}

```{r Tab9bSharingRationales, asis = TRUE}
df <- sd %>% left_join(
	readRDS("../data/generated/verbal_response_coded.rds") %>% 
		select(-exp_reflect),
	by = "id"
) %>% mutate(
	verbal_gave_reasons = !is.na(verbal_n_reasons),
	verbal_mostly_positive = 1*(verbal_positive_reasons > verbal_negative_reasons),
	exp_tment_colab = 1 - exp_tment_control,
	exp_tment_group = 2*exp_tment_colab + exp_tment_private,
	demog_active_res = as.numeric(demog_active_res) - 1,
	demog_trust = (demog_trusting + demog_trustworthy)/2
)

reasons_overall <- df %>%
	filter(verbal_gave_reasons) %>%
	summarise(across(
		starts_with(c("verbal_pos_", "verbal_neg_", "verbal_dep_")), sum
	)) %>%
	pivot_longer(
		starts_with(c("verbal_pos_", "verbal_neg_", "verbal_dep_")), values_to = "n", names_to = "reason"
	) %>% mutate(
		reason = str_remove(reason, fixed("verbal_")),
		pct_all = n/sum(df$verbal_gave_reasons) 		
	)

reasons_by_tgroup <- df %>%
	filter(verbal_gave_reasons) %>%
	group_by(exp_tment_group) %>%
	summarise(across(
		starts_with(c("verbal_pos_", "verbal_neg_", "verbal_dep_")), 
		~ {sum(.x)/n()} 
	)) %>%
	pivot_longer(
		starts_with(c("verbal_pos_", "verbal_neg_", "verbal_dep_")), values_to = "pct", names_to = "reason"
	) %>% pivot_wider(
		names_from = "exp_tment_group", names_prefix = "pct_tg", values_from = "pct"
	) %>%
	mutate(
		reason = str_remove(reason, fixed("verbal_"))
	)

reasons <- reasons_overall %>% left_join(reasons_by_tgroup, by = "reason")

tab <- reasons %>%
	mutate(
		type = str_extract(reason, "pos|neg|dep"),
		reason = paste(
			"...",
			str_remove(reason, "pos_|neg_|dep_") %>% 
				str_replace_all(fixed("_"), " ") %>%
				tolower()
		),
		across(starts_with("pct"), ~ sprintf("%.1f %%", 100 * .x))
	) %>%
	mutate(other = (reason == "... other reasons")) %>%
	arrange(type, other, -n) %>%
	select(type, everything()) %>%
	select(-other)

names(tab) <- c(
	"Rationale", "Reason", "N", 
	"% Total", "% Control/Public", "% Control/Private", 
	"% Collab/Public", "% Collab/Private"
)

tab <- rbind(
	c("I share because it ... ", rep("", 6)),
	tab %>% filter(Rationale == "pos") %>% select(-Rationale),
	c("I do not share because ... ", rep("", 6)),
	tab %>% filter(Rationale == "neg") %>% select(-Rationale),
	c("It depends ... ", rep("", 6)),
	tab %>% filter(Rationale == "dep") %>% select(-Rationale)
)

tab_latex <- gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()

sub(
	"career concerns related to failed replications or corrections",
	"career concerns related to failed replications", tab_latex
)
```

This table is based on our survey data and two separate questions. The response presented in Panel A is based on the multiple choice answer to the close-ended question "`r vars$text[60]`". Panel B is based on the answer to the open-ended question that we pose as a follow-up in our experimental scenario: "`r vars$text[7]`". The free form answers to this question have been coded by an research assistant that had no information about the experimental design or the respective treatments. In Panel A, answers from two respondents who classified their method as "Other" are included in the Total column, but excluded from the methods break-up.

```{=tex}
\setlength{\parindent}{4em}
\end{landscape}
\end{singlespace}
\newpage
```
```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```
### Table 10: Pre-Registration Across the Social Sciences {.unnumbered}

#### Panel A: Awareness {.unnumbered}

```{r Tab10aPreRegAwareSocSciResp, asis = TRUE}
tab <- create_pct_cross_tab_table(
	mr, "prereg_heard", "demog_discipline", 
	resp_label = c("No (0)", "Yes (1)"), 
	add_num_labels = FALSE, df_only = TRUE
) 
gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

#### Panel B: Take-up over Time {.unnumbered}

```{r Tab10bPreRegTimeSocSciResp, asis = TRUE}
tab <- create_pct_cross_tab_table(
	mr, "prereg_use_first_time", "demog_discipline", 
	resp_label = na.omit(sort(unique(mr$prereg_use_first_time))), 
	add_num_labels = FALSE, df_only = TRUE
) 

# NOTE: This hack Needs to be checked manually whenever something 
# the underlying data changes - which it should not

up_to_2010 <- c(
	"<= 2010", 
	sum(as.numeric(substr(tab$Accounting[1:13], 1, 1))),
	sum(as.numeric(substr(tab$Economics[1:13], 1, 1))),
	sum(as.numeric(substr(tab$`Political Science`[1:13], 1, 1))),
	sum(as.numeric(substr(tab$Psychology[1:13], 1, 1))),
	sum(as.numeric(substr(tab$Sociology[1:13], 1, 1)))
)
up_to_2010[2:6] <- sprintf(
	"%s (%.1f %%)", 
	up_to_2010[2:6],
	100*as.numeric(up_to_2010[2:6])/as.numeric(tab[26,2:6])
)
tab <- rbind(up_to_2010, tab[14:28,])

gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

```{=tex}
\newpage
\begin{landscape}
```
#### Panel C: Trustworthiness {.unnumbered}

```{r Tab10cPreRegTimeSocSciResp, asis = TRUE}
tab <- create_pct_cross_tab_table(
	mr, "prereg_opinion", "demog_discipline", 
	resp_label = get_resp_label("prereg_trust"), df_only = TRUE
) 
gt(tab ) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

This table is based on our survey data merged with the data collected by @FLCPSWMBP_2023. For Panel A, the question of our survey is "`r vars$text[95]`" where we code the response "Not at all familiar" as "No". The question used in the survey of @FLCPSWMBP_2023 is "Have you ever heard of the practice of pre-registering hypotheses or analyses in advance of a study?". For Panel B, the question of our survey is "`r vars$text[102]`". The question used in the survey of @FLCPSWMBP_2023 is "Approximately when was the first time you pre-registered hypotheses or analyses in advance of a study?". For Panel C, the question of our survey is "`r vars$text[103]`". The question used in the survey of @FLCPSWMBP_2023 is "What is your opinion of pre-registering hypotheses or analyses?". The data of @FLCPSWMBP_2023 has been collected in two waves (2018 and 2020) from a population of researchers located in the U.S. Our survey was conducted in 2022 among accounting researchers world-wide. Our survey and the merging process of the data are discussed in more detail in section 2 of the paper. The asterisks report the results of an OLS regression regressing the response on a series of field fixed effects with Accounting acting as the base field. (\*/\*\*/\*\*\*) indicate two-sided significance at the (10%/5%/1%) significant level, respecively.

```{=tex}
\setlength{\parindent}{4em}
\end{landscape}
\end{singlespace}
\newpage
```
### Table 11: Pre-Registration Across Methods {.unnumbered}

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
```
#### Panel A: Familiarity {.unnumbered}

```{r Tab11aPreRegFamiliarity, asis = TRUE}
df <- sd %>% filter(demog_method != "Other")
tab <- create_pct_cross_tab_table(
	df, "prereg_familiar", "demog_method", 
	ctests = TRUE, df_only = TRUE
) 
gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

#### Panel B: Usage {.unnumbered}

```{r Tab11bPreRegUse, asis = TRUE}

tab <- create_pct_cross_tab_table(
	df, "prereg_use", "demog_method", 
	resp_label = c("No (0)", "Yes (1)"), 
	add_num_labels = FALSE, ctests = TRUE, df_only = TRUE
) 
# Adjust levels to be zero based.
tab[4,2:6] <- t(sprintf("%.2f", as.numeric(tab[4,2:6]) - 1))
tab[5,2:6] <- t(sprintf("%.2f", as.numeric(tab[5,2:6]) - 1))
gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

#### Panel C: Trustworthiness {.unnumbered}

```{r Tab11cPreRegTrust, asis = TRUE}

tab <- create_pct_cross_tab_table(
	df, "prereg_trust", "demog_method", 
	ctests = TRUE, df_only = TRUE
) 
gt(tab) %>% 
	cols_align("right", -1) %>% 
	as_latex()
```

This table is based on our survey data and three separate questions. The response presented in Panel A is based on the question "`r vars$text[95]`", the response presented in Panel B is based on the question "`r vars$text[100]`", and the response presented in Panel C is based on the question "`r vars$text[103]`". In all Panels, answers from two respondents who classified their method as "Other" are excluded. Methods where the answers differ at the two-sided 10%-level are indicated by their first two letters.

```{=tex}
\setlength{\parindent}{4em}
\end{singlespace}
\newpage
```
## Online Supplement {.unnumbered}

The public GitHub repository at \href{https://github.com/joachim-gassen/osacc}{https://github.com/joachim-gassen/osacc} contains the data and the code of this project.

## Appendix A: Survey Administration and Instrument {.unnumbered}

```{=tex}
\begin{singlespace}
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
```
### Initial Invitation - November 21, 2022 {.unnumbered}

\[Title: Share your views on Accounting Research\]

Dear colleague,

We are conducting a study about sharing research materials, reproducibility, and pre-registration in accounting research. You have been chosen to provide your opinion on this subject because you are a published author in one of the leading accounting journals. We obtained your email address from publicly available sources.

Participating in our study takes around 7 minutes of your time. We will publicly share the anonymized data and results with the academic community in due course.

As a token of gratitude for your time, we will donate 1,000 € multiplied by the response rate to the United Nations International Children's Emergency Fund (UNICEF).

Click or copy and paste the URL below in your internet browser to participate in our study:\
\[URL\]

Should you have any other comments or questions, please feel free to contact us via \[EMAIL\].

We thank you for your consideration and participation.

Sincerely,

Joachim Gassen\
Humboldt-Universität zu Berlin\
TRR 266 Accounting for Transparency

Jacqueline Klug, Victor van Pelt\
WHU -- Otto Beisheim School of Management

Click here to unsubscribe from future e-mails. Click here to view our data protection guidelines.

Further information:

This study has been approved by the ethical review board of the WHU -- Otto Beisheim School of Management.

Participation in our study is entirely voluntary, and there are no consequences for you if you choose not to take part in it. We will ask for your consent to participate at the beginning of the study.

The data will only be analyzed and stored in anonymous form. This anonymous data will only be used for academic purposes and will be shared with other academics for research purposes.

\newpage

### Reminder 1 - November 28, 2022 {.unnumbered}

\[Title: Reminder: Share your views on Accounting Research\]

Dear colleague,

If you have not completed our **study** about **sharing research materials, reproducibility, and pre-registration in accounting research** yet, you still have time!

Participate in our **7-minute** study by following the URL below:

\[URL\]

As a token of gratitude for your time, we will **donate 1,000 € multiplied by the response rate** to the United Nations International Children's Emergency Fund (**UNICEF**).

Should you have any other comments or questions, please feel free to contact us via \[EMAIL\].

We thank you for your consideration and participation.

Sincerely,

Joachim Gassen\
Humboldt-Universität zu Berlin\
TRR 266 Accounting for Transparency

Jacqueline Klug, Victor van Pelt\
WHU -- Otto Beisheim School of Management

Click here to unsubscribe from future e-mails.

Click here to view our data protection guidelines.

Further information:

This study has been approved by the ethical review board of the WHU -- Otto Beisheim School of Management.

Participation in our study is entirely voluntary, and there are no consequences for you if you choose not to take part in it. We will ask for your consent to participate at the beginning of the study.

The data will only be analyzed and stored in anonymous form. This anonymous data will only be used for academic purposes and will be shared with other academics for research purposes.

\newpage

### Reminder 2 - December 5, 2022 {.unnumbered}

\[Title: Reminder: Share your views on Accounting Research\]

Dear colleague,

If you have not completed our **study** about **sharing research materials, reproducibility, and pre-registration in accounting research** yet, you still have time **until next Monday**. We will close the study after that date.

Participate in our **7-minute** study by following the URL below:

\[URL\]

As a token of gratitude for your time, we will **donate 1,000 € multiplied by the response rate** to the United Nations International Children's Emergency Fund (**UNICEF**).

Should you have any other comments or questions, please feel free to contact us via \[EMAIL\]

We thank you for your consideration and participation.

Sincerely,

Joachim Gassen\
Humboldt-Universität zu Berlin\
TRR 266 Accounting for Transparency

Jacqueline Klug, Victor van Pelt\
WHU -- Otto Beisheim School of Management

Click here to unsubscribe from future e-mails.

Click here to view our data protection guidelines.

Further information:

This study has been approved by the ethical review board of the WHU -- Otto Beisheim School of Management.

Participation in our study is entirely voluntary, and there are no consequences for you if you choose not to take part in it. We will ask for your consent to participate at the beginning of the study.

The data will only be analyzed and stored in anonymous form. This anonymous data will only be used for academic purposes and will be shared with other academics for research purposes.

\newpage

### Reminder 3 - December 12, 2022 {.unnumbered}

\[Title: Last Chance to Participate: Share your views on Accounting Research\]

Dear colleague,

If you have not completed our **study** about **sharing research materials, reproducibility, and pre-registration in accounting research** yet, this **Monday** is your **last chance to participate!** We will close the study by the end of today.

Participate in our **7-minute** study by following the URL below:

\[URL\]

As a token of gratitude for your time, we will **donate 1,000 € multiplied by the response rate** to the United Nations International Children's Emergency Fund (**UNICEF**).

Should you have any other comments or questions, please feel free to contact us via \[EMAIL\].

We thank you for your consideration and participation.

Sincerely,

Joachim Gassen\
Humboldt-Universität zu Berlin\
TRR 266 Accounting for Transparency

Jacqueline Klug, Victor van Pelt\
WHU -- Otto Beisheim School of Management

Click here to view our data protection guidelines.

Further information:

This study has been approved by the ethical review board of the WHU -- Otto Beisheim School of Management.

Participation in our study is entirely voluntary, and there are no consequences for you if you choose not to take part in it. We will ask for your consent to participate at the beginning of the study.

The data will only be analyzed and stored in anonymous form. This anonymous data will only be used for academic purposes and will be shared with other academics for research purposes.

\newpage

```{=tex}
\includepdf[scale=0.7, pages=1,pagecommand={\hypertarget{survey-instrument}{\subsection*{Survey Instrument}\label{survey-instrument}}\addcontentsline{toc}{subsubsection}{Survey Instrument}}]{materials/survey.pdf}
\includepdf[scale=0.7, pages=2-, pagecommand={}]{materials/survey.pdf}
```
## Appendix B: Scenario Manipulations and Instrument {.unnumbered}

### Manipulations {.unnumbered}

+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Rationale     | Private Request                                                                                                                                                                                                         | Public Request                                                                                                                                                                                                                                   |
+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Control       | You conducted a study and published it in an academic journal. Another researcher contacts you and asks for access to your research materials (i.e., the data, code, and research procedures). The researcher wants to: | You conducted a study and published it in an academic journal. Another researcher contacts you and asks you to publicly share your research materials (i.e., the data, code, and research procedures). The researcher wants to enable others to: |
|               |                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                  |
|               | -   verify how your study was conducted,                                                                                                                                                                                | -   verify how your study was conducted,                                                                                                                                                                                                         |
|               |                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                  |
|               | -   investigate whether your data and methodology are sound,                                                                                                                                                            | -   investigate whether your data and methodology are sound,                                                                                                                                                                                     |
|               |                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                  |
|               | -   control whether your findings can be reproduced, and                                                                                                                                                                | -   control whether your findings can be reproduced, and                                                                                                                                                                                         |
|               |                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                  |
|               | -   potentially conduct a new study that corrects your study.                                                                                                                                                           | -   potentially conduct a new study that corrects your study.                                                                                                                                                                                    |
+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Collaboration | You conducted a study and published it in an academic journal. Another researcher contacts you and asks for access to your research materials (i.e., the data, code, and research procedures). The researcher wants to: | You conducted a study and published it in an academic journal. Another researcher contacts you and asks you to publicly share your research materials (i.e., the data, code, and research procedures). The researcher wants to enable others to: |
|               |                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                  |
|               | -   understand how your study was conducted,                                                                                                                                                                            | -   understand how your study was conducted,                                                                                                                                                                                                     |
|               |                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                  |
|               | -   learn from your data and methodology,                                                                                                                                                                               | -   learn from your data and methodology,                                                                                                                                                                                                        |
|               |                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                  |
|               | -   contribute by generating new insights, and                                                                                                                                                                          | -   contribute by generating new insights, and                                                                                                                                                                                                   |
|               |                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                  |
|               | -   potentially conduct a new study that builds on your study.                                                                                                                                                          | -   potentially conduct a new study that builds on your study.                                                                                                                                                                                   |
+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

: {tbl-colwidths="\[20,40,40\]"}

```{=tex}
\end{singlespace}
```
```{=tex}
\includepdf[scale=0.7, pages=1,pagecommand={\hypertarget{scenario-instrument}{\subsection*{Instrument}\label{scenario-instrument}}\addcontentsline{toc}{subsubsection}{Scenario Instrument}}]{materials/scenario.pdf}
\includepdf[scale=0.7, pages=2-, pagecommand={}]{materials/scenario.pdf}
```
